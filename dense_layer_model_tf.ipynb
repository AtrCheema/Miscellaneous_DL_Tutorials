{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dense_layer_model_tf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7v0SnRASwlbWZOxS0qoFJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AtrCheema/Miscellaneous_DL_Tutorials/blob/master/dense_layer_model_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMOxocXYoTSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "\n",
        "import tempfile\n",
        "import os\n",
        "import random\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from scipy.stats.stats import linregress\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "np.set_printoptions(suppress=True) # to suppress scientific notation while printing arrays\n",
        "\n",
        "def reset_graph(seed=2):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "tf.test.is_gpu_available()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pgTozGClZ4D",
        "colab_type": "text"
      },
      "source": [
        "#### Some Utils\n",
        "\n",
        "These are utitlity functions that we will need to during this exercise to calculate differnt stuff etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AW9_7uJhtzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import random\n",
        "\n",
        "class FindErrors(object):\n",
        "\n",
        "    def __init__(self, actual, predicted, fil = None, epoch=None, meta_data='', chunk=None):\n",
        "        actual = actual.reshape(-1,)\n",
        "        predicted = predicted.reshape(-1,)\n",
        "\n",
        "        self.chunk = chunk or []\n",
        "        self.true = actual # observed/real value\n",
        "        self.predicted = predicted\n",
        "        self.fil = fil\n",
        "        self.epoch = epoch\n",
        "        self.meta_data = meta_data\n",
        "\n",
        "\n",
        "    def rmse(self):\n",
        "        return sqrt(mean_squared_error(self.true, self.predicted))\n",
        "\n",
        "    def mse(self):\n",
        "        return mean_squared_error(self.true, self.predicted)\n",
        "\n",
        "    def r2(self):\n",
        "        if self.true.ndim>1:\n",
        "            self.true = self.true.reshape(-1,)\n",
        "        if self.predicted.ndim>1:\n",
        "            self.predicted = self.predicted.reshape(-1,)\n",
        "        slope, intercept, r_value, p_value, std_err = linregress(self.true, self.predicted)\n",
        "        return r_value ** 2\n",
        "\n",
        "    def RSR(self):\n",
        "        return self.rmse() / np.std(self.true)\n",
        "\n",
        "    def NSE(self):\n",
        "        nse = 1 - sum((self.predicted - self.true) ** 2) / sum((self.true - np.mean(self.true)) ** 2)\n",
        "        return nse\n",
        "\n",
        "    def APB(self):\n",
        "        \"\"\" absolute percent bias\"\"\"\n",
        "        APB = 100.0 * sum(abs(self.predicted - self.true)) / sum(self.true)  # Absolute percent bias\n",
        "        return APB\n",
        "\n",
        "    def percent_bias(self):\n",
        "        PerBias = 100.0 * sum(self.predicted - self.true) / sum(self.true)  # percent bias\n",
        "        return PerBias\n",
        "\n",
        "    def nrmse(self):\n",
        "        \"\"\" Normalized Root Mean Squared Error \"\"\"\n",
        "        return self.rmse() / (self.true.max() - self.true.min())\n",
        "\n",
        "    def mae(self):\n",
        "        \"\"\" Mean Absolute Error \"\"\"\n",
        "        return np.mean(np.abs(self.true - self.predicted))\n",
        "\n",
        "\n",
        "class Utils(object):\n",
        "\n",
        "    # codes for some colors\n",
        "    colors = {'true': np.array([0.07233712, 0.470282, 0.24355425]),\n",
        "            'pred': np.array([0.81831849, 0.17526342, 0.4766505]),\n",
        "            'train_samples': np.array([0.9624212,  0.28477194, 0.4760216]),\n",
        "            'test_samples': np.array([0.51618149, 0.16053867, 0.45268923]),\n",
        "            'prediction': np.array([0.86537652, 0.18864455, 0.08620183]),\n",
        "            'training': np.array([0.92359535, 0.41150655, 0.42089012])\n",
        "        }\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def read_from_file(in_features=None, out_features=None, rows=None):\n",
        "        \n",
        "        df = pd.read_csv('input.csv')\n",
        "\n",
        "        cols = df.columns\n",
        "\n",
        "        ins = list(np.random.choice(cols[1:-3], size=in_features))\n",
        "        outs = list(np.random.choice(cols[-2:], size=out_features))\n",
        "\n",
        "        req_cols = ins + outs\n",
        "        df1 = df[req_cols]\n",
        "\n",
        "        return df1.values\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_data(inputs, outputs, data_len, print_lines=20):\n",
        "        in_data = np.random.random((data_len, inputs))\n",
        "        y = np.full((data_len, outputs), np.nan)\n",
        "\n",
        "        funcs = [np.sum, np.prod, np.trapz]\n",
        "\n",
        "        for out in range(out_features):\n",
        "            func = np.random.choice(funcs)\n",
        "            print('doing ', func.__name__)\n",
        "            y[:,out] = func(in_data, axis=1)\n",
        "        \n",
        "        in_data = np.hstack((in_data, y))\n",
        "\n",
        "        print(in_data[0:print_lines])  \n",
        "        print('\\n {} \\n'.format(in_data.shape))\n",
        "        print(in_data[-print_lines:])\n",
        "\n",
        "        return in_data\n",
        "\n",
        "    @staticmethod\n",
        "    def print_errors(true, pred):\n",
        "\n",
        "        er = FindErrors(true, pred)\n",
        "        for err in ['r2', 'NSE', 'APB', 'percent_bias', 'RSR', 'rmse', 'mse','nrmse', 'mae']:\n",
        "            print('{:20}  {:<10.5f} '.format(err, getattr(er, err)()))\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_scatter(true, pred, _name=None):\n",
        "        fig, ax = plt.subplots(1)\n",
        "        Utils.set_fig_dim(fig, 8,6)\n",
        "\n",
        "        ax.scatter(true, pred)\n",
        "        #plt.savefig(_name, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def random_array(_length, lower=0.0, upper=0.5):\n",
        "        \"\"\"This creates a random array of length `length` and the floats vary between `lower` and `upper`.\"\"\"\n",
        "        rand_array = np.zeros((_length))\n",
        "        for i in range(_length):\n",
        "            rand_array[i] = random.uniform(lower, upper)\n",
        "        return rand_array\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_loss(train_loss_array, test_loss_array=None, _name=' ', log=False, _path=None):\n",
        "\n",
        "        if test_loss_array:\n",
        "            ht = 10\n",
        "            _fig, (ax1, ax2) = plt.subplots(2, sharex='all')\n",
        "        else:\n",
        "            _fig, ax1 = plt.subplots(1, sharex='all')\n",
        "            ht = 5\n",
        "\n",
        "        _fig.set_figheight(ht)\n",
        "        Utils.process_axis(ax1, train_loss_array, style='-', label='training ' + _name, leg_fs=12,\n",
        "                           y_label=_name, yl_fs=14,ms=2, log=log, x_label=\"Epochs\")\n",
        "\n",
        "        if test_loss_array:\n",
        "            Utils.process_axis(ax2, test_loss_array, label='Validation ' + _name, leg_fs=12,\n",
        "                           y_label=_name, yl_fs=14, x_label='Epochs', ms=2, log=log)\n",
        "\n",
        "        _fig.suptitle('Loss variation with training epochs', fontsize=18)\n",
        "\n",
        "        #plt.savefig(_path + \"/loss_\" + _name, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def process_axis(axis,\n",
        "                    data,\n",
        "                    style= '.',\n",
        "                    x_label = \"Time\",\n",
        "                    xl_fs = 14,     # x label font size\n",
        "                    y_label=\" \",\n",
        "                    yl_fs = 14,                        # ylabel font size\n",
        "                    leg_pos=\"center left\",\n",
        "                    label = \" \",   # legend\n",
        "                    ms = 5,        # markersize\n",
        "                    leg_fs=16,\n",
        "                    leg_ms = 4,     # legend scale\n",
        "                    xtp_ls = 12, # x tick_params labelsize\n",
        "                    ytp_ls = 12, # x tick_params labelsize\n",
        "                    log= False,\n",
        "                    show_xaxis=True,\n",
        "                    top_spine = True,\n",
        "                    bottom_spine=True,\n",
        "                    invert_yaxis=False):\n",
        "\n",
        "        if label in Utils.colors:\n",
        "            c = colors[label]\n",
        "        else:\n",
        "            c = Utils.random_array(3, 0.01, 0.99)\n",
        "            print('for ', label, c)\n",
        "        \n",
        "        axis.plot(data, style, markersize=ms, color=c, label=label)\n",
        "            \n",
        "        axis.legend(loc=leg_pos, fontsize=leg_fs, markerscale=leg_ms)\n",
        "        axis.set_ylabel(y_label, fontsize=yl_fs, color=c)\n",
        "\n",
        "        if log:\n",
        "            axis.set_yscale('log')\n",
        "\n",
        "        if invert_yaxis:\n",
        "            axis.set_ylim(axis.get_ylim()[::-1])\n",
        "\n",
        "        axis.tick_params(axis=\"x\", which='major', labelsize=xtp_ls, colors=c)\n",
        "        axis.tick_params(axis=\"y\", which='major', labelsize=ytp_ls, colors='k')\n",
        "        axis.get_xaxis().set_visible(show_xaxis)\n",
        "\n",
        "        if show_xaxis:\n",
        "            axis.set_xlabel(x_label, fontsize=xl_fs, color='k')\n",
        "\n",
        "        axis.spines['top'].set_visible(top_spine)\n",
        "        axis.spines['bottom'].set_visible(bottom_spine)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def set_fig_dim(fig, width, height):\n",
        "        fig.set_figwidth(width)\n",
        "        fig.set_figheight(height)\n",
        "        return\n",
        "\n",
        "\n",
        "utils = Utils()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P1ovCRq4V2o",
        "colab_type": "text"
      },
      "source": [
        "## Model\n",
        "\n",
        "Following class facilitates to build neural model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ0wbwco4ZV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Model(object):\n",
        "\n",
        "    def __init__(self, nn_conf, data):\n",
        "        self.scalers = None\n",
        "        self.nn_conf = nn_conf\n",
        "        self.data_n = self.normalization(data)    \n",
        "\n",
        "    def normalization(self, data):\n",
        "        if self.nn_conf['normalization']:  # normalizing data\n",
        "            # container for normalized input data\n",
        "            dataset_n = np.full(data.shape, np.nan)\n",
        "            self.scalers = OrderedDict()\n",
        "\n",
        "            for dat in range(data.shape[1]):\n",
        "                value = data[:,dat]\n",
        "                val_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "                val_norm = val_scaler.fit_transform(value.reshape(-1,1))\n",
        "                dataset_n[:,dat] = val_norm.reshape(-1,)\n",
        "                self.scalers[str(dat) + '_scaler'] = val_scaler\n",
        "\n",
        "            return dataset_n\n",
        "        else:\n",
        "            return data\n",
        "\n",
        "\n",
        "    def batch_generator(self, data, args, verbose=True):\n",
        "        ins = args['in_features']\n",
        "        outs = args['out_features']\n",
        "        min_idx = args['min_ind']\n",
        "        max_idx = args['max_ind']\n",
        "        bat_size = args['batch_size']\n",
        "\n",
        "        X = data[min_idx:max_idx, 0:ins]\n",
        "        Y = data[min_idx:max_idx, -outs:]\n",
        "\n",
        "        residue = X.shape[0] % bat_size\n",
        "\n",
        "        no_of_batches = int(X.shape[0] // bat_size)\n",
        "        if verbose:\n",
        "            print('no of batches: ', no_of_batches)\n",
        "\n",
        "        if min_idx == 0:\n",
        "            self.train_batches = no_of_batches\n",
        "        else:\n",
        "            self.test_batches = no_of_batches\n",
        "\n",
        "        gen_i = 1\n",
        "        while 1:\n",
        "            gen_i +=1\n",
        "\n",
        "            st = 0\n",
        "            en = bat_size\n",
        "            for bat in range(no_of_batches):\n",
        "\n",
        "                x = X[st:en,:]\n",
        "                y = Y[st:en,:]\n",
        "\n",
        "                st += bat_size\n",
        "                en += bat_size\n",
        "\n",
        "                yield x,y \n",
        "    \n",
        "    def build_nn(self):\n",
        "        \n",
        "        in_feat = self.nn_conf['in_features']\n",
        "        lr = self.nn_conf['lr']\n",
        "        loss = self.nn_conf['loss']\n",
        "        lyrs_conf = self.nn_conf['layers_conf']\n",
        "\n",
        "\n",
        "        reset_graph()\n",
        "        k_model = Sequential()\n",
        "\n",
        "        layer = 0\n",
        "        for l,lp in lyrs_conf.items():\n",
        "            \n",
        "            if layer == 0:\n",
        "                k_model.add(lp['lyr']( units = lp['units'], activation=lp['acts'], input_shape=(in_feat,)))\n",
        "                self.add_misc_lyrs(k_model, lp['dropout'], lp['BatchNorm'])\n",
        "\n",
        "            else:\n",
        "                k_model.add(lp['lyr']( units = lp['units'], activation=lp['acts']))\n",
        "                self.add_misc_lyrs(k_model, lp['dropout'], lp['BatchNorm'])\n",
        "\n",
        "            layer += 1\n",
        "\n",
        "        optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
        "            name='Adam'\n",
        "        )\n",
        "\n",
        "\n",
        "        k_model.compile(loss=nn_conf['loss'],\n",
        "                    optimizer=optimizer)\n",
        "        \n",
        "        self.k_model = k_model\n",
        "\n",
        "        return k_model\n",
        "    \n",
        "    @staticmethod\n",
        "    def add_misc_lyrs(k_model, dropout=None, BatchNorm=False):\n",
        "        if dropout:\n",
        "            k_model.add(Dropout(dropout))\n",
        "        if BatchNorm:\n",
        "            k_model.add(BatchNormalization())\n",
        "        return\n",
        "\n",
        "    \n",
        "    def train_nn(self, train_args):\n",
        "    \n",
        "        train_gen = self.batch_generator(self.data_n, train_args)\n",
        "        next(train_gen)\n",
        "        train_gen = self.batch_generator(self.data_n, train_args, verbose=False)\n",
        "\n",
        "        epochs = self.nn_conf['training_epochs']\n",
        "\n",
        "        history = self.k_model.fit_generator(train_gen,\n",
        "                                    steps_per_epoch=self.train_batches,\n",
        "                                    epochs=epochs)\n",
        "        return history\n",
        "\n",
        "\n",
        "    def test_nn(self, args, mode='test'):\n",
        "\n",
        "        out_feat = self.nn_conf['out_features']\n",
        "\n",
        "        gen = self.batch_generator(self.data_n, args)\n",
        "        next(gen)\n",
        "        gen = self.batch_generator(self.data_n, args, verbose=False)\n",
        "\n",
        "        # get number of batches\n",
        "        batches = self.test_batches if mode=='test' else self.train_batches\n",
        "\n",
        "        true = np.full((int(batches*nn_conf['batch_size']), out_feat), np.nan)\n",
        "\n",
        "        st = 0\n",
        "        en = nn_conf['batch_size']\n",
        "        for i in range(batches):\n",
        "            _, true[st:en,:] = next(gen) \n",
        "            st += nn_conf['batch_size']\n",
        "            en += nn_conf['batch_size']\n",
        "\n",
        "        gen = self.batch_generator(self.data_n, args, verbose=False)\n",
        "        predictions = self.k_model.predict_generator(gen,\n",
        "                                                   steps=batches)  \n",
        "        \n",
        "        if self.nn_conf['normalization']:\n",
        "            true, predictions = self.denormalize_y(true, predictions)\n",
        "        \n",
        "         \n",
        "        return true, predictions\n",
        "    \n",
        "\n",
        "    def denormalize_y(self, true, pred):\n",
        "        out_feat = self.nn_conf['out_features']\n",
        "\n",
        "        st_idx = len(self.scalers) - out_features\n",
        "        for i in range(out_features):\n",
        "\n",
        "            idx = st_idx + i\n",
        "            scaler_name = str(list(self.scalers.keys())[idx])\n",
        "            print(scaler_name, 'scaler name')\n",
        "            \n",
        "            y_scaler = self.scalers[scaler_name]  \n",
        "            true[:,i] = y_scaler.inverse_transform(true[:,i].reshape(-1, 1)).reshape(-1,)\n",
        "            pred[:,i] = y_scaler.inverse_transform(pred[:,i].reshape(-1, 1)).reshape(-1,)\n",
        "        \n",
        "        return true, pred\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVEA2Ftqxm1I",
        "colab_type": "text"
      },
      "source": [
        "## Getting Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz8dHk_Pob-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rows = 1000\n",
        "in_features = 4\n",
        "out_features = 2\n",
        "\n",
        "# if 'input.csv' in os.listdir(os.getcwd()):\n",
        "#     data = utils.read_from_file(in_features, out_features, rows)\n",
        "# else:\n",
        "#     data = utils.generate_data(in_features, out_features, rows)\n",
        "\n",
        "data = utils.generate_data(in_features, out_features, rows, 2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCH504B77aKM",
        "colab_type": "text"
      },
      "source": [
        "### Building Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGZDXeY3J3aS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "*****************************************************************************\n",
        "             Change values in this cell and see model performacne\n",
        "*****************************************************************************\n",
        "\"\"\"\n",
        "\n",
        "nn_conf = {\n",
        "    'batch_size': 4,\n",
        "    'lr': 0.00001,  # learning rate\n",
        "    'loss': 'mse',  \n",
        "    'training_epochs': 50,\n",
        "    'normalization': True\n",
        "}\n",
        "\n",
        "lyrs = OrderedDict()\n",
        "# you can add or remove layers with the exception that last layer must have units = out_features\n",
        "# acts: activations, can be None or 'relu', 'tanh', 'sigmoid', 'elu'\n",
        "# dropout can be any fractional value or None\n",
        "#BatchNorm can be True or False\n",
        "\n",
        "lyrs[0] = {'lyr': Dense, 'units': 64, 'acts': None, 'dropout': 0.2, 'BatchNorm': False}\n",
        "lyrs[1] = {'lyr': Dense, 'units': 32, 'acts': None, 'dropout': 0.2, 'BatchNorm': False}\n",
        "#lyrs[2] ={'lyr': Dense, 'units': 16, 'acts': None, 'dropout': 0.2, 'BatchNorm': False}\n",
        "lyrs[3] = {'lyr': Dense, 'units': out_features, 'acts': None, 'dropout': None, 'BatchNorm': False}\n",
        "\n",
        "\n",
        "# percentage of data to be considered as training data\n",
        "# remaining data will be considered as test data\n",
        "training_percent = 0.7  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu3I-kRGqi5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "st_ind = 0\n",
        "end_ind = int(data.shape[0] * training_percent)\n",
        "\n",
        "train_args = {\n",
        "    'batch_size': nn_conf['batch_size'],\n",
        "    'min_ind': st_ind,\n",
        "    'max_ind': end_ind,\n",
        "    'in_features': in_features,\n",
        "    'out_features': out_features\n",
        "}\n",
        "\n",
        "test_args = train_args.copy()\n",
        "test_args['min_ind'] = end_ind\n",
        "test_args['max_ind'] = data.shape[0]\n",
        "\n",
        "\n",
        "nn_conf['layers_conf'] = lyrs\n",
        "nn_conf['in_features'] = in_features\n",
        "nn_conf['out_features'] = out_features\n",
        "\n",
        "\n",
        "\n",
        "model = Model(nn_conf, data=data)\n",
        "\n",
        "keras_model = model.build_nn()\n",
        "\n",
        "keras_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HIgDPZlUSXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x,y = next(train_gen)\n",
        "# print(x.shape, y.shape)\n",
        "# print(x,'\\n', y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Upi6k94dWXxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x,y = next(test_gen)\n",
        "# print(x.shape, y.shape)\n",
        "# print(x,'\\n', y)\n",
        "#model.scalers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnq9TAGH6_6R",
        "colab_type": "text"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCFLCvk2sAwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.train_nn( train_args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW28mLU3ItZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "utils.plot_loss(history.history['loss'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us__EdJK7FRZ",
        "colab_type": "text"
      },
      "source": [
        "### Make predictions\n",
        "\n",
        "We evaluate our model on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPSISNAGawXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_true, test_predictions = model.test_nn(test_args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzCRJhAqfwaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(out_features):\n",
        "    utils.plot_scatter(test_true[:,i], test_predictions[:,i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_jyEynIixW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(out_features):\n",
        "    utils.print_errors(test_true[:,i], test_predictions[:,i])\n",
        "    print('\\n\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ADo-hmB_5aC",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate on Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86JY26UQlrzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_true, train_predictions = model.test_nn(train_args, 'training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp4Sg3zFAm4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(out_features):\n",
        "    utils.plot_scatter(train_true[:,i], train_predictions[:,i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBIAtyVtAt1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(out_features):\n",
        "    utils.print_errors(train_true[:,i], train_predictions[:,i])\n",
        "    print('\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}