{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Srtqrlwr09Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "np.set_printoptions(suppress=True) # to suppress scientific notation while printing arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data\n",
    "Create a data which is supposed to represent a timeseries prediction problem. The data has 6 columns and 1000 columns. The first five columns are supposed to be input and the last column is supposed to be output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0  2000  4000  6000  8000 10000]\n",
      " [    1  2001  4001  6001  8001 10001]\n",
      " [    2  2002  4002  6002  8002 10002]\n",
      " [    3  2003  4003  6003  8003 10003]\n",
      " [    4  2004  4004  6004  8004 10004]\n",
      " [    5  2005  4005  6005  8005 10005]\n",
      " [    6  2006  4006  6006  8006 10006]\n",
      " [    7  2007  4007  6007  8007 10007]\n",
      " [    8  2008  4008  6008  8008 10008]\n",
      " [    9  2009  4009  6009  8009 10009]\n",
      " [   10  2010  4010  6010  8010 10010]\n",
      " [   11  2011  4011  6011  8011 10011]\n",
      " [   12  2012  4012  6012  8012 10012]\n",
      " [   13  2013  4013  6013  8013 10013]\n",
      " [   14  2014  4014  6014  8014 10014]\n",
      " [   15  2015  4015  6015  8015 10015]\n",
      " [   16  2016  4016  6016  8016 10016]\n",
      " [   17  2017  4017  6017  8017 10017]\n",
      " [   18  2018  4018  6018  8018 10018]\n",
      " [   19  2019  4019  6019  8019 10019]]\n",
      "\n",
      " (2000, 6) \n",
      "\n",
      "[[ 1980  3980  5980  7980  9980 11980]\n",
      " [ 1981  3981  5981  7981  9981 11981]\n",
      " [ 1982  3982  5982  7982  9982 11982]\n",
      " [ 1983  3983  5983  7983  9983 11983]\n",
      " [ 1984  3984  5984  7984  9984 11984]\n",
      " [ 1985  3985  5985  7985  9985 11985]\n",
      " [ 1986  3986  5986  7986  9986 11986]\n",
      " [ 1987  3987  5987  7987  9987 11987]\n",
      " [ 1988  3988  5988  7988  9988 11988]\n",
      " [ 1989  3989  5989  7989  9989 11989]\n",
      " [ 1990  3990  5990  7990  9990 11990]\n",
      " [ 1991  3991  5991  7991  9991 11991]\n",
      " [ 1992  3992  5992  7992  9992 11992]\n",
      " [ 1993  3993  5993  7993  9993 11993]\n",
      " [ 1994  3994  5994  7994  9994 11994]\n",
      " [ 1995  3995  5995  7995  9995 11995]\n",
      " [ 1996  3996  5996  7996  9996 11996]\n",
      " [ 1997  3997  5997  7997  9997 11997]\n",
      " [ 1998  3998  5998  7998  9998 11998]\n",
      " [ 1999  3999  5999  7999  9999 11999]]\n"
     ]
    }
   ],
   "source": [
    "rows = 2000\n",
    "cols = 6\n",
    "data = np.arange(int(rows*cols)).reshape(-1,rows).transpose()\n",
    "print(data[0:20])  \n",
    "print('\\n {} \\n'.format(data.shape))\n",
    "print(data[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x data: (581, 7, 5) \n",
      "shape of y data: (587, 1)\n",
      ".\n",
      "12 values are skipped from start and 7 values are skipped from end in output array\n",
      "\n",
      "potential samples are 581\n",
      "\n",
      "residue is 5 \n",
      "\n",
      "Actual samples are 576\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176 192 208 224 240 256 272\n",
      " 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560\n",
      " 576]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 192 208 224 240 256 272\n",
      " 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560\n",
      " 576 581] \n",
      "\n",
      "Number of batches are 36 \n",
      "\n",
      "shape of batches for:\n",
      "x_data   y_data\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "shape of x data: (181, 7, 5) \n",
      "shape of y data: (187, 1)\n",
      ".\n",
      "12 values are skipped from start and 7 values are skipped from end in output array\n",
      "\n",
      "potential samples are 181\n",
      "\n",
      "residue is 5 \n",
      "\n",
      "Actual samples are 176\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 181] \n",
      "\n",
      "Number of batches are 11 \n",
      "\n",
      "shape of batches for:\n",
      "x_data   y_data\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n"
     ]
    }
   ],
   "source": [
    "def first_nan_from_end(ar):\n",
    "    \"\"\" \n",
    "    This function finds index for first nan from the group which is present at the end of array.\n",
    "    [np.nan, np.nan, 0,2,3,0,3, np.nan, np.nan, np.nan, np.nan] >> 7\n",
    "    [np.nan, np.nan, 1,2,3,0, np.nan, np.nan, np.nan] >> 6\n",
    "    [0,2,3,0,3] >> 5\n",
    "    [np.nan, np.nan, 0,2,3,0,3] >> 7    \n",
    "    \"\"\"\n",
    "    last_non_zero=0\n",
    "    \n",
    "    for idx, val in enumerate(ar[::-1]):\n",
    "        if ~np.isnan(val): # val >= 0:\n",
    "            last_non_zero = idx\n",
    "            break\n",
    "    return ar.shape[0] - last_non_zero    \n",
    "    \n",
    "\n",
    "def batch_generator(data, lookback, in_features, out_features, batch_size, step, min_ind, max_ind, future_y_val,\n",
    "                   trim_last_batch=True):\n",
    "    \"\"\"\n",
    "    :param data: `ndarray`, input data.\n",
    "    :param lookback: `int`, sequence length, number of values LSTM will see at time `t` to make prediction at `t+1`.\n",
    "    :in_features: `int`, number of columns in `data` starting from 0 to be considered as input\n",
    "    :out_features: `int`, number of columns in `data` started from last to be considred as output/prediction.\n",
    "    :parm trim_last_batch: bool, if True, last batch will be ignored if that contains samples less than `batch_size`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # selecting the data of interest for x and y    \n",
    "    X = data[min_ind:max_ind, 0:in_features]\n",
    "    Y = data[min_ind:max_ind, -out_features].reshape(-1,out_features)\n",
    "    \n",
    "    # container for keeping x and y windows. A `windows` is here defined as one complete set of data at one timestep.\n",
    "    x_wins = np.full((X.shape[0], lookback, in_features), np.nan, dtype=np.float32)\n",
    "    y_wins = np.full((Y.shape[0], out_features), np.nan)\n",
    "    \n",
    "    # creating windows from X data\n",
    "    st = lookback*step - step                 # starting point of sampling from data\n",
    "    for j in range(st, X.shape[0]-lookback):\n",
    "        en = j - lookback*step\n",
    "        indices = np.arange(j, en, -step)\n",
    "        ind = np.flip(indices)\n",
    "        x_wins[j,:,:] = X[ind,:]\n",
    "\n",
    "    # creating windows from Y data\n",
    "    for i in range(0, Y.shape[0]-lookback):\n",
    "        y_wins[i,:] = Y[i+lookback,:]\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"removing trailing nans\"\"\"\n",
    "    first_nan_at_end = first_nan_from_end(y_wins)  # first nan in last part of data, start skipping from here\n",
    "    y_wins = y_wins[0:first_nan_at_end,:]\n",
    "    x_wins = x_wins[0:first_nan_at_end,:]\n",
    "\n",
    "    \"\"\"removing nans from start\"\"\"\n",
    "    y_val = st-lookback + future_y_val\n",
    "    if st>0:\n",
    "        x_wins = x_wins[st:,:]\n",
    "        y_wins = y_wins[y_val:,:]\n",
    "\n",
    "\n",
    "    print(\"\"\"shape of x data: {} \\nshape of y data: {}\"\"\".format(x_wins.shape, y_wins.shape))\n",
    "\n",
    "\n",
    "    print(\"\"\".\\n{} values are skipped from start and {} values are skipped from end in output array\"\"\"\n",
    "          .format(st, X.shape[0]-first_nan_at_end))\n",
    "\n",
    "    pot_samples = x_wins.shape[0]\n",
    "\n",
    "    print('\\npotential samples are {}'.format(pot_samples))\n",
    "\n",
    "    residue = pot_samples % batch_size\n",
    "    print('\\nresidue is {} '.format(residue))\n",
    "\n",
    "    samples = pot_samples - residue\n",
    "    print('\\nActual samples are {}'.format(samples))\n",
    "\n",
    "    interval = np.arange(0, samples + batch_size, batch_size)\n",
    "    print('\\nPotential intervals: {}'.format(interval ))\n",
    "\n",
    "    if residue > 0:\n",
    "        interval = np.append(interval, pot_samples)\n",
    "    print('\\nActual interval: {} '.format(interval))\n",
    "\n",
    "    if trim_last_batch:\n",
    "        no_of_batches = len(interval)-2\n",
    "    else:\n",
    "        no_of_batches = len(interval) - 1 \n",
    "        \n",
    "    print('\\nNumber of batches are {} '.format(no_of_batches))\n",
    "\n",
    "\n",
    "    x_batches = np.full((no_of_batches, batch_size, lookback, in_features), np.nan)\n",
    "    y_batches = np.full((no_of_batches, batch_size, out_features), np.nan)\n",
    "\n",
    "\n",
    "    for b in range(no_of_batches):\n",
    "        st = interval[b]\n",
    "        en = interval[b + 1]\n",
    "        an_x_batch = x_wins[st:en, :, :]\n",
    "        x_batches[b] = an_x_batch\n",
    "       # y_batches[b] = y_wins[st:en]\n",
    "        y_batches[b] = y_wins[st+1:en+1]\n",
    "\n",
    "\n",
    "    print('\\nshape of batches for:')\n",
    "    print('x_data ', ' y_data')\n",
    "    for i,j in zip(x_batches, y_batches):\n",
    "        ishp, jshp = None, None\n",
    "        if isinstance(i, np.ndarray):\n",
    "            ishp = i.shape\n",
    "        if isinstance(j, np.ndarray):\n",
    "            jshp = j.shape\n",
    "        print(ishp, jshp)\n",
    "    \n",
    "    return x_batches, y_batches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_lookback=7  # sequence length\n",
    "input_features = 5\n",
    "output_features = 1\n",
    "_batch_size = 16\n",
    "input_stepsize = 2\n",
    "st_ind = 0\n",
    "end_ind = 600\n",
    "t_plus_ith_val = 1 # which value to predict in future, e.g if input is 11,12,13,14 and default value of this variable means we\n",
    "                  # want to predict 15, setting value equal to 3 means we want to predict 17.\n",
    "\n",
    "train_x_batches, train_y_batches = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                                    input_stepsize, st_ind, end_ind, t_plus_ith_val,\n",
    "                                    trim_last_batch = True)            \n",
    "\n",
    "test_x_batches, test_y_batches = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                                    input_stepsize,\n",
    "                                    min_ind = 600,\n",
    "                                    max_ind = 800,\n",
    "                                    future_y_val = t_plus_ith_val,\n",
    "                                    trim_last_batch = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first train batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0. 2000. 4000. 6000. 8000.]\n",
      " [   2. 2002. 4002. 6002. 8002.]] [10003.] \n",
      "\n",
      "[[   1. 2001. 4001. 6001. 8001.]\n",
      " [   3. 2003. 4003. 6003. 8003.]] [10004.] \n",
      "\n",
      "[[   2. 2002. 4002. 6002. 8002.]\n",
      " [   4. 2004. 4004. 6004. 8004.]] [10005.] \n",
      "\n",
      "[[   3. 2003. 4003. 6003. 8003.]\n",
      " [   5. 2005. 4005. 6005. 8005.]] [10006.] \n",
      "\n",
      "[[   4. 2004. 4004. 6004. 8004.]\n",
      " [   6. 2006. 4006. 6006. 8006.]] [10007.] \n",
      "\n",
      "[[   5. 2005. 4005. 6005. 8005.]\n",
      " [   7. 2007. 4007. 6007. 8007.]] [10008.] \n",
      "\n",
      "[[   6. 2006. 4006. 6006. 8006.]\n",
      " [   8. 2008. 4008. 6008. 8008.]] [10009.] \n",
      "\n",
      "[[   7. 2007. 4007. 6007. 8007.]\n",
      " [   9. 2009. 4009. 6009. 8009.]] [10010.] \n",
      "\n",
      "[[   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]] [10011.] \n",
      "\n",
      "[[   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]] [10012.] \n",
      "\n",
      "[[  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]] [10013.] \n",
      "\n",
      "[[  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]] [10014.] \n",
      "\n",
      "[[  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]] [10015.] \n",
      "\n",
      "[[  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]] [10016.] \n",
      "\n",
      "[[  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]] [10017.] \n",
      "\n",
      "[[  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]] [10018.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(train_x_batches[0], train_y_batches[0]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second train batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  16. 1016. 2016. 3016. 4016.]\n",
      " [  18. 1018. 2018. 3018. 4018.]\n",
      " [  20. 1020. 2020. 3020. 4020.]\n",
      " [  22. 1022. 2022. 3022. 4022.]\n",
      " [  24. 1024. 2024. 3024. 4024.]\n",
      " [  26. 1026. 2026. 3026. 4026.]\n",
      " [  28. 1028. 2028. 3028. 4028.]] [5030.] \n",
      "\n",
      "[[  17. 1017. 2017. 3017. 4017.]\n",
      " [  19. 1019. 2019. 3019. 4019.]\n",
      " [  21. 1021. 2021. 3021. 4021.]\n",
      " [  23. 1023. 2023. 3023. 4023.]\n",
      " [  25. 1025. 2025. 3025. 4025.]\n",
      " [  27. 1027. 2027. 3027. 4027.]\n",
      " [  29. 1029. 2029. 3029. 4029.]] [5031.] \n",
      "\n",
      "[[  18. 1018. 2018. 3018. 4018.]\n",
      " [  20. 1020. 2020. 3020. 4020.]\n",
      " [  22. 1022. 2022. 3022. 4022.]\n",
      " [  24. 1024. 2024. 3024. 4024.]\n",
      " [  26. 1026. 2026. 3026. 4026.]\n",
      " [  28. 1028. 2028. 3028. 4028.]\n",
      " [  30. 1030. 2030. 3030. 4030.]] [5032.] \n",
      "\n",
      "[[  19. 1019. 2019. 3019. 4019.]\n",
      " [  21. 1021. 2021. 3021. 4021.]\n",
      " [  23. 1023. 2023. 3023. 4023.]\n",
      " [  25. 1025. 2025. 3025. 4025.]\n",
      " [  27. 1027. 2027. 3027. 4027.]\n",
      " [  29. 1029. 2029. 3029. 4029.]\n",
      " [  31. 1031. 2031. 3031. 4031.]] [5033.] \n",
      "\n",
      "[[  20. 1020. 2020. 3020. 4020.]\n",
      " [  22. 1022. 2022. 3022. 4022.]\n",
      " [  24. 1024. 2024. 3024. 4024.]\n",
      " [  26. 1026. 2026. 3026. 4026.]\n",
      " [  28. 1028. 2028. 3028. 4028.]\n",
      " [  30. 1030. 2030. 3030. 4030.]\n",
      " [  32. 1032. 2032. 3032. 4032.]] [5034.] \n",
      "\n",
      "[[  21. 1021. 2021. 3021. 4021.]\n",
      " [  23. 1023. 2023. 3023. 4023.]\n",
      " [  25. 1025. 2025. 3025. 4025.]\n",
      " [  27. 1027. 2027. 3027. 4027.]\n",
      " [  29. 1029. 2029. 3029. 4029.]\n",
      " [  31. 1031. 2031. 3031. 4031.]\n",
      " [  33. 1033. 2033. 3033. 4033.]] [5035.] \n",
      "\n",
      "[[  22. 1022. 2022. 3022. 4022.]\n",
      " [  24. 1024. 2024. 3024. 4024.]\n",
      " [  26. 1026. 2026. 3026. 4026.]\n",
      " [  28. 1028. 2028. 3028. 4028.]\n",
      " [  30. 1030. 2030. 3030. 4030.]\n",
      " [  32. 1032. 2032. 3032. 4032.]\n",
      " [  34. 1034. 2034. 3034. 4034.]] [5036.] \n",
      "\n",
      "[[  23. 1023. 2023. 3023. 4023.]\n",
      " [  25. 1025. 2025. 3025. 4025.]\n",
      " [  27. 1027. 2027. 3027. 4027.]\n",
      " [  29. 1029. 2029. 3029. 4029.]\n",
      " [  31. 1031. 2031. 3031. 4031.]\n",
      " [  33. 1033. 2033. 3033. 4033.]\n",
      " [  35. 1035. 2035. 3035. 4035.]] [5037.] \n",
      "\n",
      "[[  24. 1024. 2024. 3024. 4024.]\n",
      " [  26. 1026. 2026. 3026. 4026.]\n",
      " [  28. 1028. 2028. 3028. 4028.]\n",
      " [  30. 1030. 2030. 3030. 4030.]\n",
      " [  32. 1032. 2032. 3032. 4032.]\n",
      " [  34. 1034. 2034. 3034. 4034.]\n",
      " [  36. 1036. 2036. 3036. 4036.]] [5038.] \n",
      "\n",
      "[[  25. 1025. 2025. 3025. 4025.]\n",
      " [  27. 1027. 2027. 3027. 4027.]\n",
      " [  29. 1029. 2029. 3029. 4029.]\n",
      " [  31. 1031. 2031. 3031. 4031.]\n",
      " [  33. 1033. 2033. 3033. 4033.]\n",
      " [  35. 1035. 2035. 3035. 4035.]\n",
      " [  37. 1037. 2037. 3037. 4037.]] [5039.] \n",
      "\n",
      "[[  26. 1026. 2026. 3026. 4026.]\n",
      " [  28. 1028. 2028. 3028. 4028.]\n",
      " [  30. 1030. 2030. 3030. 4030.]\n",
      " [  32. 1032. 2032. 3032. 4032.]\n",
      " [  34. 1034. 2034. 3034. 4034.]\n",
      " [  36. 1036. 2036. 3036. 4036.]\n",
      " [  38. 1038. 2038. 3038. 4038.]] [5040.] \n",
      "\n",
      "[[  27. 1027. 2027. 3027. 4027.]\n",
      " [  29. 1029. 2029. 3029. 4029.]\n",
      " [  31. 1031. 2031. 3031. 4031.]\n",
      " [  33. 1033. 2033. 3033. 4033.]\n",
      " [  35. 1035. 2035. 3035. 4035.]\n",
      " [  37. 1037. 2037. 3037. 4037.]\n",
      " [  39. 1039. 2039. 3039. 4039.]] [5041.] \n",
      "\n",
      "[[  28. 1028. 2028. 3028. 4028.]\n",
      " [  30. 1030. 2030. 3030. 4030.]\n",
      " [  32. 1032. 2032. 3032. 4032.]\n",
      " [  34. 1034. 2034. 3034. 4034.]\n",
      " [  36. 1036. 2036. 3036. 4036.]\n",
      " [  38. 1038. 2038. 3038. 4038.]\n",
      " [  40. 1040. 2040. 3040. 4040.]] [5042.] \n",
      "\n",
      "[[  29. 1029. 2029. 3029. 4029.]\n",
      " [  31. 1031. 2031. 3031. 4031.]\n",
      " [  33. 1033. 2033. 3033. 4033.]\n",
      " [  35. 1035. 2035. 3035. 4035.]\n",
      " [  37. 1037. 2037. 3037. 4037.]\n",
      " [  39. 1039. 2039. 3039. 4039.]\n",
      " [  41. 1041. 2041. 3041. 4041.]] [5043.] \n",
      "\n",
      "[[  30. 1030. 2030. 3030. 4030.]\n",
      " [  32. 1032. 2032. 3032. 4032.]\n",
      " [  34. 1034. 2034. 3034. 4034.]\n",
      " [  36. 1036. 2036. 3036. 4036.]\n",
      " [  38. 1038. 2038. 3038. 4038.]\n",
      " [  40. 1040. 2040. 3040. 4040.]\n",
      " [  42. 1042. 2042. 3042. 4042.]] [5044.] \n",
      "\n",
      "[[  31. 1031. 2031. 3031. 4031.]\n",
      " [  33. 1033. 2033. 3033. 4033.]\n",
      " [  35. 1035. 2035. 3035. 4035.]\n",
      " [  37. 1037. 2037. 3037. 4037.]\n",
      " [  39. 1039. 2039. 3039. 4039.]\n",
      " [  41. 1041. 2041. 3041. 4041.]\n",
      " [  43. 1043. 2043. 3043. 4043.]] [5045.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(train_x_batches[1], train_y_batches[1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last train batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 560. 1560. 2560. 3560. 4560.]\n",
      " [ 562. 1562. 2562. 3562. 4562.]\n",
      " [ 564. 1564. 2564. 3564. 4564.]\n",
      " [ 566. 1566. 2566. 3566. 4566.]\n",
      " [ 568. 1568. 2568. 3568. 4568.]\n",
      " [ 570. 1570. 2570. 3570. 4570.]\n",
      " [ 572. 1572. 2572. 3572. 4572.]] [5574.] \n",
      "\n",
      "[[ 561. 1561. 2561. 3561. 4561.]\n",
      " [ 563. 1563. 2563. 3563. 4563.]\n",
      " [ 565. 1565. 2565. 3565. 4565.]\n",
      " [ 567. 1567. 2567. 3567. 4567.]\n",
      " [ 569. 1569. 2569. 3569. 4569.]\n",
      " [ 571. 1571. 2571. 3571. 4571.]\n",
      " [ 573. 1573. 2573. 3573. 4573.]] [5575.] \n",
      "\n",
      "[[ 562. 1562. 2562. 3562. 4562.]\n",
      " [ 564. 1564. 2564. 3564. 4564.]\n",
      " [ 566. 1566. 2566. 3566. 4566.]\n",
      " [ 568. 1568. 2568. 3568. 4568.]\n",
      " [ 570. 1570. 2570. 3570. 4570.]\n",
      " [ 572. 1572. 2572. 3572. 4572.]\n",
      " [ 574. 1574. 2574. 3574. 4574.]] [5576.] \n",
      "\n",
      "[[ 563. 1563. 2563. 3563. 4563.]\n",
      " [ 565. 1565. 2565. 3565. 4565.]\n",
      " [ 567. 1567. 2567. 3567. 4567.]\n",
      " [ 569. 1569. 2569. 3569. 4569.]\n",
      " [ 571. 1571. 2571. 3571. 4571.]\n",
      " [ 573. 1573. 2573. 3573. 4573.]\n",
      " [ 575. 1575. 2575. 3575. 4575.]] [5577.] \n",
      "\n",
      "[[ 564. 1564. 2564. 3564. 4564.]\n",
      " [ 566. 1566. 2566. 3566. 4566.]\n",
      " [ 568. 1568. 2568. 3568. 4568.]\n",
      " [ 570. 1570. 2570. 3570. 4570.]\n",
      " [ 572. 1572. 2572. 3572. 4572.]\n",
      " [ 574. 1574. 2574. 3574. 4574.]\n",
      " [ 576. 1576. 2576. 3576. 4576.]] [5578.] \n",
      "\n",
      "[[ 565. 1565. 2565. 3565. 4565.]\n",
      " [ 567. 1567. 2567. 3567. 4567.]\n",
      " [ 569. 1569. 2569. 3569. 4569.]\n",
      " [ 571. 1571. 2571. 3571. 4571.]\n",
      " [ 573. 1573. 2573. 3573. 4573.]\n",
      " [ 575. 1575. 2575. 3575. 4575.]\n",
      " [ 577. 1577. 2577. 3577. 4577.]] [5579.] \n",
      "\n",
      "[[ 566. 1566. 2566. 3566. 4566.]\n",
      " [ 568. 1568. 2568. 3568. 4568.]\n",
      " [ 570. 1570. 2570. 3570. 4570.]\n",
      " [ 572. 1572. 2572. 3572. 4572.]\n",
      " [ 574. 1574. 2574. 3574. 4574.]\n",
      " [ 576. 1576. 2576. 3576. 4576.]\n",
      " [ 578. 1578. 2578. 3578. 4578.]] [5580.] \n",
      "\n",
      "[[ 567. 1567. 2567. 3567. 4567.]\n",
      " [ 569. 1569. 2569. 3569. 4569.]\n",
      " [ 571. 1571. 2571. 3571. 4571.]\n",
      " [ 573. 1573. 2573. 3573. 4573.]\n",
      " [ 575. 1575. 2575. 3575. 4575.]\n",
      " [ 577. 1577. 2577. 3577. 4577.]\n",
      " [ 579. 1579. 2579. 3579. 4579.]] [5581.] \n",
      "\n",
      "[[ 568. 1568. 2568. 3568. 4568.]\n",
      " [ 570. 1570. 2570. 3570. 4570.]\n",
      " [ 572. 1572. 2572. 3572. 4572.]\n",
      " [ 574. 1574. 2574. 3574. 4574.]\n",
      " [ 576. 1576. 2576. 3576. 4576.]\n",
      " [ 578. 1578. 2578. 3578. 4578.]\n",
      " [ 580. 1580. 2580. 3580. 4580.]] [5582.] \n",
      "\n",
      "[[ 569. 1569. 2569. 3569. 4569.]\n",
      " [ 571. 1571. 2571. 3571. 4571.]\n",
      " [ 573. 1573. 2573. 3573. 4573.]\n",
      " [ 575. 1575. 2575. 3575. 4575.]\n",
      " [ 577. 1577. 2577. 3577. 4577.]\n",
      " [ 579. 1579. 2579. 3579. 4579.]\n",
      " [ 581. 1581. 2581. 3581. 4581.]] [5583.] \n",
      "\n",
      "[[ 570. 1570. 2570. 3570. 4570.]\n",
      " [ 572. 1572. 2572. 3572. 4572.]\n",
      " [ 574. 1574. 2574. 3574. 4574.]\n",
      " [ 576. 1576. 2576. 3576. 4576.]\n",
      " [ 578. 1578. 2578. 3578. 4578.]\n",
      " [ 580. 1580. 2580. 3580. 4580.]\n",
      " [ 582. 1582. 2582. 3582. 4582.]] [5584.] \n",
      "\n",
      "[[ 571. 1571. 2571. 3571. 4571.]\n",
      " [ 573. 1573. 2573. 3573. 4573.]\n",
      " [ 575. 1575. 2575. 3575. 4575.]\n",
      " [ 577. 1577. 2577. 3577. 4577.]\n",
      " [ 579. 1579. 2579. 3579. 4579.]\n",
      " [ 581. 1581. 2581. 3581. 4581.]\n",
      " [ 583. 1583. 2583. 3583. 4583.]] [5585.] \n",
      "\n",
      "[[ 572. 1572. 2572. 3572. 4572.]\n",
      " [ 574. 1574. 2574. 3574. 4574.]\n",
      " [ 576. 1576. 2576. 3576. 4576.]\n",
      " [ 578. 1578. 2578. 3578. 4578.]\n",
      " [ 580. 1580. 2580. 3580. 4580.]\n",
      " [ 582. 1582. 2582. 3582. 4582.]\n",
      " [ 584. 1584. 2584. 3584. 4584.]] [5586.] \n",
      "\n",
      "[[ 573. 1573. 2573. 3573. 4573.]\n",
      " [ 575. 1575. 2575. 3575. 4575.]\n",
      " [ 577. 1577. 2577. 3577. 4577.]\n",
      " [ 579. 1579. 2579. 3579. 4579.]\n",
      " [ 581. 1581. 2581. 3581. 4581.]\n",
      " [ 583. 1583. 2583. 3583. 4583.]\n",
      " [ 585. 1585. 2585. 3585. 4585.]] [5587.] \n",
      "\n",
      "[[ 574. 1574. 2574. 3574. 4574.]\n",
      " [ 576. 1576. 2576. 3576. 4576.]\n",
      " [ 578. 1578. 2578. 3578. 4578.]\n",
      " [ 580. 1580. 2580. 3580. 4580.]\n",
      " [ 582. 1582. 2582. 3582. 4582.]\n",
      " [ 584. 1584. 2584. 3584. 4584.]\n",
      " [ 586. 1586. 2586. 3586. 4586.]] [5588.] \n",
      "\n",
      "[[ 575. 1575. 2575. 3575. 4575.]\n",
      " [ 577. 1577. 2577. 3577. 4577.]\n",
      " [ 579. 1579. 2579. 3579. 4579.]\n",
      " [ 581. 1581. 2581. 3581. 4581.]\n",
      " [ 583. 1583. 2583. 3583. 4583.]\n",
      " [ 585. 1585. 2585. 3585. 4585.]\n",
      " [ 587. 1587. 2587. 3587. 4587.]] [5589.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(train_x_batches[-1], train_y_batches[-1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 600. 1600. 2600. 3600. 4600.]\n",
      " [ 602. 1602. 2602. 3602. 4602.]\n",
      " [ 604. 1604. 2604. 3604. 4604.]\n",
      " [ 606. 1606. 2606. 3606. 4606.]\n",
      " [ 608. 1608. 2608. 3608. 4608.]\n",
      " [ 610. 1610. 2610. 3610. 4610.]\n",
      " [ 612. 1612. 2612. 3612. 4612.]] [5614.] \n",
      "\n",
      "[[ 601. 1601. 2601. 3601. 4601.]\n",
      " [ 603. 1603. 2603. 3603. 4603.]\n",
      " [ 605. 1605. 2605. 3605. 4605.]\n",
      " [ 607. 1607. 2607. 3607. 4607.]\n",
      " [ 609. 1609. 2609. 3609. 4609.]\n",
      " [ 611. 1611. 2611. 3611. 4611.]\n",
      " [ 613. 1613. 2613. 3613. 4613.]] [5615.] \n",
      "\n",
      "[[ 602. 1602. 2602. 3602. 4602.]\n",
      " [ 604. 1604. 2604. 3604. 4604.]\n",
      " [ 606. 1606. 2606. 3606. 4606.]\n",
      " [ 608. 1608. 2608. 3608. 4608.]\n",
      " [ 610. 1610. 2610. 3610. 4610.]\n",
      " [ 612. 1612. 2612. 3612. 4612.]\n",
      " [ 614. 1614. 2614. 3614. 4614.]] [5616.] \n",
      "\n",
      "[[ 603. 1603. 2603. 3603. 4603.]\n",
      " [ 605. 1605. 2605. 3605. 4605.]\n",
      " [ 607. 1607. 2607. 3607. 4607.]\n",
      " [ 609. 1609. 2609. 3609. 4609.]\n",
      " [ 611. 1611. 2611. 3611. 4611.]\n",
      " [ 613. 1613. 2613. 3613. 4613.]\n",
      " [ 615. 1615. 2615. 3615. 4615.]] [5617.] \n",
      "\n",
      "[[ 604. 1604. 2604. 3604. 4604.]\n",
      " [ 606. 1606. 2606. 3606. 4606.]\n",
      " [ 608. 1608. 2608. 3608. 4608.]\n",
      " [ 610. 1610. 2610. 3610. 4610.]\n",
      " [ 612. 1612. 2612. 3612. 4612.]\n",
      " [ 614. 1614. 2614. 3614. 4614.]\n",
      " [ 616. 1616. 2616. 3616. 4616.]] [5618.] \n",
      "\n",
      "[[ 605. 1605. 2605. 3605. 4605.]\n",
      " [ 607. 1607. 2607. 3607. 4607.]\n",
      " [ 609. 1609. 2609. 3609. 4609.]\n",
      " [ 611. 1611. 2611. 3611. 4611.]\n",
      " [ 613. 1613. 2613. 3613. 4613.]\n",
      " [ 615. 1615. 2615. 3615. 4615.]\n",
      " [ 617. 1617. 2617. 3617. 4617.]] [5619.] \n",
      "\n",
      "[[ 606. 1606. 2606. 3606. 4606.]\n",
      " [ 608. 1608. 2608. 3608. 4608.]\n",
      " [ 610. 1610. 2610. 3610. 4610.]\n",
      " [ 612. 1612. 2612. 3612. 4612.]\n",
      " [ 614. 1614. 2614. 3614. 4614.]\n",
      " [ 616. 1616. 2616. 3616. 4616.]\n",
      " [ 618. 1618. 2618. 3618. 4618.]] [5620.] \n",
      "\n",
      "[[ 607. 1607. 2607. 3607. 4607.]\n",
      " [ 609. 1609. 2609. 3609. 4609.]\n",
      " [ 611. 1611. 2611. 3611. 4611.]\n",
      " [ 613. 1613. 2613. 3613. 4613.]\n",
      " [ 615. 1615. 2615. 3615. 4615.]\n",
      " [ 617. 1617. 2617. 3617. 4617.]\n",
      " [ 619. 1619. 2619. 3619. 4619.]] [5621.] \n",
      "\n",
      "[[ 608. 1608. 2608. 3608. 4608.]\n",
      " [ 610. 1610. 2610. 3610. 4610.]\n",
      " [ 612. 1612. 2612. 3612. 4612.]\n",
      " [ 614. 1614. 2614. 3614. 4614.]\n",
      " [ 616. 1616. 2616. 3616. 4616.]\n",
      " [ 618. 1618. 2618. 3618. 4618.]\n",
      " [ 620. 1620. 2620. 3620. 4620.]] [5622.] \n",
      "\n",
      "[[ 609. 1609. 2609. 3609. 4609.]\n",
      " [ 611. 1611. 2611. 3611. 4611.]\n",
      " [ 613. 1613. 2613. 3613. 4613.]\n",
      " [ 615. 1615. 2615. 3615. 4615.]\n",
      " [ 617. 1617. 2617. 3617. 4617.]\n",
      " [ 619. 1619. 2619. 3619. 4619.]\n",
      " [ 621. 1621. 2621. 3621. 4621.]] [5623.] \n",
      "\n",
      "[[ 610. 1610. 2610. 3610. 4610.]\n",
      " [ 612. 1612. 2612. 3612. 4612.]\n",
      " [ 614. 1614. 2614. 3614. 4614.]\n",
      " [ 616. 1616. 2616. 3616. 4616.]\n",
      " [ 618. 1618. 2618. 3618. 4618.]\n",
      " [ 620. 1620. 2620. 3620. 4620.]\n",
      " [ 622. 1622. 2622. 3622. 4622.]] [5624.] \n",
      "\n",
      "[[ 611. 1611. 2611. 3611. 4611.]\n",
      " [ 613. 1613. 2613. 3613. 4613.]\n",
      " [ 615. 1615. 2615. 3615. 4615.]\n",
      " [ 617. 1617. 2617. 3617. 4617.]\n",
      " [ 619. 1619. 2619. 3619. 4619.]\n",
      " [ 621. 1621. 2621. 3621. 4621.]\n",
      " [ 623. 1623. 2623. 3623. 4623.]] [5625.] \n",
      "\n",
      "[[ 612. 1612. 2612. 3612. 4612.]\n",
      " [ 614. 1614. 2614. 3614. 4614.]\n",
      " [ 616. 1616. 2616. 3616. 4616.]\n",
      " [ 618. 1618. 2618. 3618. 4618.]\n",
      " [ 620. 1620. 2620. 3620. 4620.]\n",
      " [ 622. 1622. 2622. 3622. 4622.]\n",
      " [ 624. 1624. 2624. 3624. 4624.]] [5626.] \n",
      "\n",
      "[[ 613. 1613. 2613. 3613. 4613.]\n",
      " [ 615. 1615. 2615. 3615. 4615.]\n",
      " [ 617. 1617. 2617. 3617. 4617.]\n",
      " [ 619. 1619. 2619. 3619. 4619.]\n",
      " [ 621. 1621. 2621. 3621. 4621.]\n",
      " [ 623. 1623. 2623. 3623. 4623.]\n",
      " [ 625. 1625. 2625. 3625. 4625.]] [5627.] \n",
      "\n",
      "[[ 614. 1614. 2614. 3614. 4614.]\n",
      " [ 616. 1616. 2616. 3616. 4616.]\n",
      " [ 618. 1618. 2618. 3618. 4618.]\n",
      " [ 620. 1620. 2620. 3620. 4620.]\n",
      " [ 622. 1622. 2622. 3622. 4622.]\n",
      " [ 624. 1624. 2624. 3624. 4624.]\n",
      " [ 626. 1626. 2626. 3626. 4626.]] [5628.] \n",
      "\n",
      "[[ 615. 1615. 2615. 3615. 4615.]\n",
      " [ 617. 1617. 2617. 3617. 4617.]\n",
      " [ 619. 1619. 2619. 3619. 4619.]\n",
      " [ 621. 1621. 2621. 3621. 4621.]\n",
      " [ 623. 1623. 2623. 3623. 4623.]\n",
      " [ 625. 1625. 2625. 3625. 4625.]\n",
      " [ 627. 1627. 2627. 3627. 4627.]] [5629.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[0], test_y_batches[0]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 616. 1616. 2616. 3616. 4616.]\n",
      " [ 618. 1618. 2618. 3618. 4618.]\n",
      " [ 620. 1620. 2620. 3620. 4620.]\n",
      " [ 622. 1622. 2622. 3622. 4622.]\n",
      " [ 624. 1624. 2624. 3624. 4624.]\n",
      " [ 626. 1626. 2626. 3626. 4626.]\n",
      " [ 628. 1628. 2628. 3628. 4628.]] [5630.] \n",
      "\n",
      "[[ 617. 1617. 2617. 3617. 4617.]\n",
      " [ 619. 1619. 2619. 3619. 4619.]\n",
      " [ 621. 1621. 2621. 3621. 4621.]\n",
      " [ 623. 1623. 2623. 3623. 4623.]\n",
      " [ 625. 1625. 2625. 3625. 4625.]\n",
      " [ 627. 1627. 2627. 3627. 4627.]\n",
      " [ 629. 1629. 2629. 3629. 4629.]] [5631.] \n",
      "\n",
      "[[ 618. 1618. 2618. 3618. 4618.]\n",
      " [ 620. 1620. 2620. 3620. 4620.]\n",
      " [ 622. 1622. 2622. 3622. 4622.]\n",
      " [ 624. 1624. 2624. 3624. 4624.]\n",
      " [ 626. 1626. 2626. 3626. 4626.]\n",
      " [ 628. 1628. 2628. 3628. 4628.]\n",
      " [ 630. 1630. 2630. 3630. 4630.]] [5632.] \n",
      "\n",
      "[[ 619. 1619. 2619. 3619. 4619.]\n",
      " [ 621. 1621. 2621. 3621. 4621.]\n",
      " [ 623. 1623. 2623. 3623. 4623.]\n",
      " [ 625. 1625. 2625. 3625. 4625.]\n",
      " [ 627. 1627. 2627. 3627. 4627.]\n",
      " [ 629. 1629. 2629. 3629. 4629.]\n",
      " [ 631. 1631. 2631. 3631. 4631.]] [5633.] \n",
      "\n",
      "[[ 620. 1620. 2620. 3620. 4620.]\n",
      " [ 622. 1622. 2622. 3622. 4622.]\n",
      " [ 624. 1624. 2624. 3624. 4624.]\n",
      " [ 626. 1626. 2626. 3626. 4626.]\n",
      " [ 628. 1628. 2628. 3628. 4628.]\n",
      " [ 630. 1630. 2630. 3630. 4630.]\n",
      " [ 632. 1632. 2632. 3632. 4632.]] [5634.] \n",
      "\n",
      "[[ 621. 1621. 2621. 3621. 4621.]\n",
      " [ 623. 1623. 2623. 3623. 4623.]\n",
      " [ 625. 1625. 2625. 3625. 4625.]\n",
      " [ 627. 1627. 2627. 3627. 4627.]\n",
      " [ 629. 1629. 2629. 3629. 4629.]\n",
      " [ 631. 1631. 2631. 3631. 4631.]\n",
      " [ 633. 1633. 2633. 3633. 4633.]] [5635.] \n",
      "\n",
      "[[ 622. 1622. 2622. 3622. 4622.]\n",
      " [ 624. 1624. 2624. 3624. 4624.]\n",
      " [ 626. 1626. 2626. 3626. 4626.]\n",
      " [ 628. 1628. 2628. 3628. 4628.]\n",
      " [ 630. 1630. 2630. 3630. 4630.]\n",
      " [ 632. 1632. 2632. 3632. 4632.]\n",
      " [ 634. 1634. 2634. 3634. 4634.]] [5636.] \n",
      "\n",
      "[[ 623. 1623. 2623. 3623. 4623.]\n",
      " [ 625. 1625. 2625. 3625. 4625.]\n",
      " [ 627. 1627. 2627. 3627. 4627.]\n",
      " [ 629. 1629. 2629. 3629. 4629.]\n",
      " [ 631. 1631. 2631. 3631. 4631.]\n",
      " [ 633. 1633. 2633. 3633. 4633.]\n",
      " [ 635. 1635. 2635. 3635. 4635.]] [5637.] \n",
      "\n",
      "[[ 624. 1624. 2624. 3624. 4624.]\n",
      " [ 626. 1626. 2626. 3626. 4626.]\n",
      " [ 628. 1628. 2628. 3628. 4628.]\n",
      " [ 630. 1630. 2630. 3630. 4630.]\n",
      " [ 632. 1632. 2632. 3632. 4632.]\n",
      " [ 634. 1634. 2634. 3634. 4634.]\n",
      " [ 636. 1636. 2636. 3636. 4636.]] [5638.] \n",
      "\n",
      "[[ 625. 1625. 2625. 3625. 4625.]\n",
      " [ 627. 1627. 2627. 3627. 4627.]\n",
      " [ 629. 1629. 2629. 3629. 4629.]\n",
      " [ 631. 1631. 2631. 3631. 4631.]\n",
      " [ 633. 1633. 2633. 3633. 4633.]\n",
      " [ 635. 1635. 2635. 3635. 4635.]\n",
      " [ 637. 1637. 2637. 3637. 4637.]] [5639.] \n",
      "\n",
      "[[ 626. 1626. 2626. 3626. 4626.]\n",
      " [ 628. 1628. 2628. 3628. 4628.]\n",
      " [ 630. 1630. 2630. 3630. 4630.]\n",
      " [ 632. 1632. 2632. 3632. 4632.]\n",
      " [ 634. 1634. 2634. 3634. 4634.]\n",
      " [ 636. 1636. 2636. 3636. 4636.]\n",
      " [ 638. 1638. 2638. 3638. 4638.]] [5640.] \n",
      "\n",
      "[[ 627. 1627. 2627. 3627. 4627.]\n",
      " [ 629. 1629. 2629. 3629. 4629.]\n",
      " [ 631. 1631. 2631. 3631. 4631.]\n",
      " [ 633. 1633. 2633. 3633. 4633.]\n",
      " [ 635. 1635. 2635. 3635. 4635.]\n",
      " [ 637. 1637. 2637. 3637. 4637.]\n",
      " [ 639. 1639. 2639. 3639. 4639.]] [5641.] \n",
      "\n",
      "[[ 628. 1628. 2628. 3628. 4628.]\n",
      " [ 630. 1630. 2630. 3630. 4630.]\n",
      " [ 632. 1632. 2632. 3632. 4632.]\n",
      " [ 634. 1634. 2634. 3634. 4634.]\n",
      " [ 636. 1636. 2636. 3636. 4636.]\n",
      " [ 638. 1638. 2638. 3638. 4638.]\n",
      " [ 640. 1640. 2640. 3640. 4640.]] [5642.] \n",
      "\n",
      "[[ 629. 1629. 2629. 3629. 4629.]\n",
      " [ 631. 1631. 2631. 3631. 4631.]\n",
      " [ 633. 1633. 2633. 3633. 4633.]\n",
      " [ 635. 1635. 2635. 3635. 4635.]\n",
      " [ 637. 1637. 2637. 3637. 4637.]\n",
      " [ 639. 1639. 2639. 3639. 4639.]\n",
      " [ 641. 1641. 2641. 3641. 4641.]] [5643.] \n",
      "\n",
      "[[ 630. 1630. 2630. 3630. 4630.]\n",
      " [ 632. 1632. 2632. 3632. 4632.]\n",
      " [ 634. 1634. 2634. 3634. 4634.]\n",
      " [ 636. 1636. 2636. 3636. 4636.]\n",
      " [ 638. 1638. 2638. 3638. 4638.]\n",
      " [ 640. 1640. 2640. 3640. 4640.]\n",
      " [ 642. 1642. 2642. 3642. 4642.]] [5644.] \n",
      "\n",
      "[[ 631. 1631. 2631. 3631. 4631.]\n",
      " [ 633. 1633. 2633. 3633. 4633.]\n",
      " [ 635. 1635. 2635. 3635. 4635.]\n",
      " [ 637. 1637. 2637. 3637. 4637.]\n",
      " [ 639. 1639. 2639. 3639. 4639.]\n",
      " [ 641. 1641. 2641. 3641. 4641.]\n",
      " [ 643. 1643. 2643. 3643. 4643.]] [5645.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[1], test_y_batches[1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second last test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 744. 1744. 2744. 3744. 4744.]\n",
      " [ 746. 1746. 2746. 3746. 4746.]\n",
      " [ 748. 1748. 2748. 3748. 4748.]\n",
      " [ 750. 1750. 2750. 3750. 4750.]\n",
      " [ 752. 1752. 2752. 3752. 4752.]\n",
      " [ 754. 1754. 2754. 3754. 4754.]\n",
      " [ 756. 1756. 2756. 3756. 4756.]] [5758.] \n",
      "\n",
      "[[ 745. 1745. 2745. 3745. 4745.]\n",
      " [ 747. 1747. 2747. 3747. 4747.]\n",
      " [ 749. 1749. 2749. 3749. 4749.]\n",
      " [ 751. 1751. 2751. 3751. 4751.]\n",
      " [ 753. 1753. 2753. 3753. 4753.]\n",
      " [ 755. 1755. 2755. 3755. 4755.]\n",
      " [ 757. 1757. 2757. 3757. 4757.]] [5759.] \n",
      "\n",
      "[[ 746. 1746. 2746. 3746. 4746.]\n",
      " [ 748. 1748. 2748. 3748. 4748.]\n",
      " [ 750. 1750. 2750. 3750. 4750.]\n",
      " [ 752. 1752. 2752. 3752. 4752.]\n",
      " [ 754. 1754. 2754. 3754. 4754.]\n",
      " [ 756. 1756. 2756. 3756. 4756.]\n",
      " [ 758. 1758. 2758. 3758. 4758.]] [5760.] \n",
      "\n",
      "[[ 747. 1747. 2747. 3747. 4747.]\n",
      " [ 749. 1749. 2749. 3749. 4749.]\n",
      " [ 751. 1751. 2751. 3751. 4751.]\n",
      " [ 753. 1753. 2753. 3753. 4753.]\n",
      " [ 755. 1755. 2755. 3755. 4755.]\n",
      " [ 757. 1757. 2757. 3757. 4757.]\n",
      " [ 759. 1759. 2759. 3759. 4759.]] [5761.] \n",
      "\n",
      "[[ 748. 1748. 2748. 3748. 4748.]\n",
      " [ 750. 1750. 2750. 3750. 4750.]\n",
      " [ 752. 1752. 2752. 3752. 4752.]\n",
      " [ 754. 1754. 2754. 3754. 4754.]\n",
      " [ 756. 1756. 2756. 3756. 4756.]\n",
      " [ 758. 1758. 2758. 3758. 4758.]\n",
      " [ 760. 1760. 2760. 3760. 4760.]] [5762.] \n",
      "\n",
      "[[ 749. 1749. 2749. 3749. 4749.]\n",
      " [ 751. 1751. 2751. 3751. 4751.]\n",
      " [ 753. 1753. 2753. 3753. 4753.]\n",
      " [ 755. 1755. 2755. 3755. 4755.]\n",
      " [ 757. 1757. 2757. 3757. 4757.]\n",
      " [ 759. 1759. 2759. 3759. 4759.]\n",
      " [ 761. 1761. 2761. 3761. 4761.]] [5763.] \n",
      "\n",
      "[[ 750. 1750. 2750. 3750. 4750.]\n",
      " [ 752. 1752. 2752. 3752. 4752.]\n",
      " [ 754. 1754. 2754. 3754. 4754.]\n",
      " [ 756. 1756. 2756. 3756. 4756.]\n",
      " [ 758. 1758. 2758. 3758. 4758.]\n",
      " [ 760. 1760. 2760. 3760. 4760.]\n",
      " [ 762. 1762. 2762. 3762. 4762.]] [5764.] \n",
      "\n",
      "[[ 751. 1751. 2751. 3751. 4751.]\n",
      " [ 753. 1753. 2753. 3753. 4753.]\n",
      " [ 755. 1755. 2755. 3755. 4755.]\n",
      " [ 757. 1757. 2757. 3757. 4757.]\n",
      " [ 759. 1759. 2759. 3759. 4759.]\n",
      " [ 761. 1761. 2761. 3761. 4761.]\n",
      " [ 763. 1763. 2763. 3763. 4763.]] [5765.] \n",
      "\n",
      "[[ 752. 1752. 2752. 3752. 4752.]\n",
      " [ 754. 1754. 2754. 3754. 4754.]\n",
      " [ 756. 1756. 2756. 3756. 4756.]\n",
      " [ 758. 1758. 2758. 3758. 4758.]\n",
      " [ 760. 1760. 2760. 3760. 4760.]\n",
      " [ 762. 1762. 2762. 3762. 4762.]\n",
      " [ 764. 1764. 2764. 3764. 4764.]] [5766.] \n",
      "\n",
      "[[ 753. 1753. 2753. 3753. 4753.]\n",
      " [ 755. 1755. 2755. 3755. 4755.]\n",
      " [ 757. 1757. 2757. 3757. 4757.]\n",
      " [ 759. 1759. 2759. 3759. 4759.]\n",
      " [ 761. 1761. 2761. 3761. 4761.]\n",
      " [ 763. 1763. 2763. 3763. 4763.]\n",
      " [ 765. 1765. 2765. 3765. 4765.]] [5767.] \n",
      "\n",
      "[[ 754. 1754. 2754. 3754. 4754.]\n",
      " [ 756. 1756. 2756. 3756. 4756.]\n",
      " [ 758. 1758. 2758. 3758. 4758.]\n",
      " [ 760. 1760. 2760. 3760. 4760.]\n",
      " [ 762. 1762. 2762. 3762. 4762.]\n",
      " [ 764. 1764. 2764. 3764. 4764.]\n",
      " [ 766. 1766. 2766. 3766. 4766.]] [5768.] \n",
      "\n",
      "[[ 755. 1755. 2755. 3755. 4755.]\n",
      " [ 757. 1757. 2757. 3757. 4757.]\n",
      " [ 759. 1759. 2759. 3759. 4759.]\n",
      " [ 761. 1761. 2761. 3761. 4761.]\n",
      " [ 763. 1763. 2763. 3763. 4763.]\n",
      " [ 765. 1765. 2765. 3765. 4765.]\n",
      " [ 767. 1767. 2767. 3767. 4767.]] [5769.] \n",
      "\n",
      "[[ 756. 1756. 2756. 3756. 4756.]\n",
      " [ 758. 1758. 2758. 3758. 4758.]\n",
      " [ 760. 1760. 2760. 3760. 4760.]\n",
      " [ 762. 1762. 2762. 3762. 4762.]\n",
      " [ 764. 1764. 2764. 3764. 4764.]\n",
      " [ 766. 1766. 2766. 3766. 4766.]\n",
      " [ 768. 1768. 2768. 3768. 4768.]] [5770.] \n",
      "\n",
      "[[ 757. 1757. 2757. 3757. 4757.]\n",
      " [ 759. 1759. 2759. 3759. 4759.]\n",
      " [ 761. 1761. 2761. 3761. 4761.]\n",
      " [ 763. 1763. 2763. 3763. 4763.]\n",
      " [ 765. 1765. 2765. 3765. 4765.]\n",
      " [ 767. 1767. 2767. 3767. 4767.]\n",
      " [ 769. 1769. 2769. 3769. 4769.]] [5771.] \n",
      "\n",
      "[[ 758. 1758. 2758. 3758. 4758.]\n",
      " [ 760. 1760. 2760. 3760. 4760.]\n",
      " [ 762. 1762. 2762. 3762. 4762.]\n",
      " [ 764. 1764. 2764. 3764. 4764.]\n",
      " [ 766. 1766. 2766. 3766. 4766.]\n",
      " [ 768. 1768. 2768. 3768. 4768.]\n",
      " [ 770. 1770. 2770. 3770. 4770.]] [5772.] \n",
      "\n",
      "[[ 759. 1759. 2759. 3759. 4759.]\n",
      " [ 761. 1761. 2761. 3761. 4761.]\n",
      " [ 763. 1763. 2763. 3763. 4763.]\n",
      " [ 765. 1765. 2765. 3765. 4765.]\n",
      " [ 767. 1767. 2767. 3767. 4767.]\n",
      " [ 769. 1769. 2769. 3769. 4769.]\n",
      " [ 771. 1771. 2771. 3771. 4771.]] [5773.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[-2], test_y_batches[-2]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 760. 1760. 2760. 3760. 4760.]\n",
      " [ 762. 1762. 2762. 3762. 4762.]\n",
      " [ 764. 1764. 2764. 3764. 4764.]\n",
      " [ 766. 1766. 2766. 3766. 4766.]\n",
      " [ 768. 1768. 2768. 3768. 4768.]\n",
      " [ 770. 1770. 2770. 3770. 4770.]\n",
      " [ 772. 1772. 2772. 3772. 4772.]] [5774.] \n",
      "\n",
      "[[ 761. 1761. 2761. 3761. 4761.]\n",
      " [ 763. 1763. 2763. 3763. 4763.]\n",
      " [ 765. 1765. 2765. 3765. 4765.]\n",
      " [ 767. 1767. 2767. 3767. 4767.]\n",
      " [ 769. 1769. 2769. 3769. 4769.]\n",
      " [ 771. 1771. 2771. 3771. 4771.]\n",
      " [ 773. 1773. 2773. 3773. 4773.]] [5775.] \n",
      "\n",
      "[[ 762. 1762. 2762. 3762. 4762.]\n",
      " [ 764. 1764. 2764. 3764. 4764.]\n",
      " [ 766. 1766. 2766. 3766. 4766.]\n",
      " [ 768. 1768. 2768. 3768. 4768.]\n",
      " [ 770. 1770. 2770. 3770. 4770.]\n",
      " [ 772. 1772. 2772. 3772. 4772.]\n",
      " [ 774. 1774. 2774. 3774. 4774.]] [5776.] \n",
      "\n",
      "[[ 763. 1763. 2763. 3763. 4763.]\n",
      " [ 765. 1765. 2765. 3765. 4765.]\n",
      " [ 767. 1767. 2767. 3767. 4767.]\n",
      " [ 769. 1769. 2769. 3769. 4769.]\n",
      " [ 771. 1771. 2771. 3771. 4771.]\n",
      " [ 773. 1773. 2773. 3773. 4773.]\n",
      " [ 775. 1775. 2775. 3775. 4775.]] [5777.] \n",
      "\n",
      "[[ 764. 1764. 2764. 3764. 4764.]\n",
      " [ 766. 1766. 2766. 3766. 4766.]\n",
      " [ 768. 1768. 2768. 3768. 4768.]\n",
      " [ 770. 1770. 2770. 3770. 4770.]\n",
      " [ 772. 1772. 2772. 3772. 4772.]\n",
      " [ 774. 1774. 2774. 3774. 4774.]\n",
      " [ 776. 1776. 2776. 3776. 4776.]] [5778.] \n",
      "\n",
      "[[ 765. 1765. 2765. 3765. 4765.]\n",
      " [ 767. 1767. 2767. 3767. 4767.]\n",
      " [ 769. 1769. 2769. 3769. 4769.]\n",
      " [ 771. 1771. 2771. 3771. 4771.]\n",
      " [ 773. 1773. 2773. 3773. 4773.]\n",
      " [ 775. 1775. 2775. 3775. 4775.]\n",
      " [ 777. 1777. 2777. 3777. 4777.]] [5779.] \n",
      "\n",
      "[[ 766. 1766. 2766. 3766. 4766.]\n",
      " [ 768. 1768. 2768. 3768. 4768.]\n",
      " [ 770. 1770. 2770. 3770. 4770.]\n",
      " [ 772. 1772. 2772. 3772. 4772.]\n",
      " [ 774. 1774. 2774. 3774. 4774.]\n",
      " [ 776. 1776. 2776. 3776. 4776.]\n",
      " [ 778. 1778. 2778. 3778. 4778.]] [5780.] \n",
      "\n",
      "[[ 767. 1767. 2767. 3767. 4767.]\n",
      " [ 769. 1769. 2769. 3769. 4769.]\n",
      " [ 771. 1771. 2771. 3771. 4771.]\n",
      " [ 773. 1773. 2773. 3773. 4773.]\n",
      " [ 775. 1775. 2775. 3775. 4775.]\n",
      " [ 777. 1777. 2777. 3777. 4777.]\n",
      " [ 779. 1779. 2779. 3779. 4779.]] [5781.] \n",
      "\n",
      "[[ 768. 1768. 2768. 3768. 4768.]\n",
      " [ 770. 1770. 2770. 3770. 4770.]\n",
      " [ 772. 1772. 2772. 3772. 4772.]\n",
      " [ 774. 1774. 2774. 3774. 4774.]\n",
      " [ 776. 1776. 2776. 3776. 4776.]\n",
      " [ 778. 1778. 2778. 3778. 4778.]\n",
      " [ 780. 1780. 2780. 3780. 4780.]] [5782.] \n",
      "\n",
      "[[ 769. 1769. 2769. 3769. 4769.]\n",
      " [ 771. 1771. 2771. 3771. 4771.]\n",
      " [ 773. 1773. 2773. 3773. 4773.]\n",
      " [ 775. 1775. 2775. 3775. 4775.]\n",
      " [ 777. 1777. 2777. 3777. 4777.]\n",
      " [ 779. 1779. 2779. 3779. 4779.]\n",
      " [ 781. 1781. 2781. 3781. 4781.]] [5783.] \n",
      "\n",
      "[[ 770. 1770. 2770. 3770. 4770.]\n",
      " [ 772. 1772. 2772. 3772. 4772.]\n",
      " [ 774. 1774. 2774. 3774. 4774.]\n",
      " [ 776. 1776. 2776. 3776. 4776.]\n",
      " [ 778. 1778. 2778. 3778. 4778.]\n",
      " [ 780. 1780. 2780. 3780. 4780.]\n",
      " [ 782. 1782. 2782. 3782. 4782.]] [5784.] \n",
      "\n",
      "[[ 771. 1771. 2771. 3771. 4771.]\n",
      " [ 773. 1773. 2773. 3773. 4773.]\n",
      " [ 775. 1775. 2775. 3775. 4775.]\n",
      " [ 777. 1777. 2777. 3777. 4777.]\n",
      " [ 779. 1779. 2779. 3779. 4779.]\n",
      " [ 781. 1781. 2781. 3781. 4781.]\n",
      " [ 783. 1783. 2783. 3783. 4783.]] [5785.] \n",
      "\n",
      "[[ 772. 1772. 2772. 3772. 4772.]\n",
      " [ 774. 1774. 2774. 3774. 4774.]\n",
      " [ 776. 1776. 2776. 3776. 4776.]\n",
      " [ 778. 1778. 2778. 3778. 4778.]\n",
      " [ 780. 1780. 2780. 3780. 4780.]\n",
      " [ 782. 1782. 2782. 3782. 4782.]\n",
      " [ 784. 1784. 2784. 3784. 4784.]] [5786.] \n",
      "\n",
      "[[ 773. 1773. 2773. 3773. 4773.]\n",
      " [ 775. 1775. 2775. 3775. 4775.]\n",
      " [ 777. 1777. 2777. 3777. 4777.]\n",
      " [ 779. 1779. 2779. 3779. 4779.]\n",
      " [ 781. 1781. 2781. 3781. 4781.]\n",
      " [ 783. 1783. 2783. 3783. 4783.]\n",
      " [ 785. 1785. 2785. 3785. 4785.]] [5787.] \n",
      "\n",
      "[[ 774. 1774. 2774. 3774. 4774.]\n",
      " [ 776. 1776. 2776. 3776. 4776.]\n",
      " [ 778. 1778. 2778. 3778. 4778.]\n",
      " [ 780. 1780. 2780. 3780. 4780.]\n",
      " [ 782. 1782. 2782. 3782. 4782.]\n",
      " [ 784. 1784. 2784. 3784. 4784.]\n",
      " [ 786. 1786. 2786. 3786. 4786.]] [5788.] \n",
      "\n",
      "[[ 775. 1775. 2775. 3775. 4775.]\n",
      " [ 777. 1777. 2777. 3777. 4777.]\n",
      " [ 779. 1779. 2779. 3779. 4779.]\n",
      " [ 781. 1781. 2781. 3781. 4781.]\n",
      " [ 783. 1783. 2783. 3783. 4783.]\n",
      " [ 785. 1785. 2785. 3785. 4785.]\n",
      " [ 787. 1787. 2787. 3787. 4787.]] [5789.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[-1], test_y_batches[-1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Generator using `yield`\n",
    "Instead of using `return` statement, we can use `yield` statement, which is more memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    " \n",
    "\n",
    "def batch_generator(data, lookback, in_features, out_features, batch_size, step, min_ind, max_ind, future_y_val,\n",
    "                   norm=False, trim_last_batch=True):\n",
    "    \"\"\"\n",
    "    :param data: `ndarray`, input data.\n",
    "    :param lookback: `int`, sequence length, number of values LSTM will see at time `t` to make prediction at `t+1`.\n",
    "    :in_features: `int`, number of columns in `data` starting from 0 to be considered as input\n",
    "    :out_features: `int`, number of columns in `data` started from last to be considred as output/prediction.\n",
    "    :parm trim_last_batch: bool, if True, last batch will be ignored if that contains samples less than `batch_size`.\n",
    "    \"\"\"\n",
    "\n",
    "    # selecting the data of interest for x and y    \n",
    "    X = data[min_ind:max_ind, 0:in_features]\n",
    "    Y = data[min_ind:max_ind, -out_features].reshape(-1,out_features)\n",
    "    \n",
    "    if norm:\n",
    "        X = scaler.fit_transform(X)\n",
    "        Y = scaler.fit_transform(Y)\n",
    "    \n",
    "    # container for keeping x and y windows. A `windows` is here defined as one complete set of data at one timestep.\n",
    "    x_wins = np.full((X.shape[0], lookback, in_features), np.nan, dtype=np.float32)\n",
    "    y_wins = np.full((Y.shape[0], out_features), np.nan)\n",
    "\n",
    "    # creating windows from X data\n",
    "    st = lookback*step - step # starting point of sampling from data\n",
    "    for j in range(st, X.shape[0]-lookback):\n",
    "        en = j - lookback*step\n",
    "        indices = np.arange(j, en, -step)\n",
    "        ind = np.flip(indices)\n",
    "        x_wins[j,:,:] = X[ind,:]\n",
    "\n",
    "    # creating windows from Y data\n",
    "    for i in range(0, Y.shape[0]-lookback):\n",
    "        y_wins[i,:] = Y[i+lookback,:]\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"removing trailing nans\"\"\"\n",
    "    first_nan_at_end = first_nan_from_end(y_wins)  # first nan in last part of data, start skipping from here\n",
    "    y_wins = y_wins[0:first_nan_at_end,:]\n",
    "    x_wins = x_wins[0:first_nan_at_end,:]\n",
    "\n",
    "    \"\"\"removing nans from start\"\"\"\n",
    "    y_val = st-lookback + future_y_val\n",
    "    if st>0:\n",
    "        x_wins = x_wins[st:,:]\n",
    "        y_wins = y_wins[y_val:,:]    \n",
    "\n",
    "    print(\"\"\"shape of x data: {} \\nshape of y data: {}\"\"\".format(x_wins.shape, y_wins.shape))\n",
    "\n",
    "    print(\"\"\".\\n{} values are skipped from start and {} values are skipped from end in output array\"\"\"\n",
    "          .format(st, X.shape[0]-first_nan_at_end))\n",
    "\n",
    "    pot_samples = x_wins.shape[0]\n",
    "\n",
    "    print('\\npotential samples are {}'.format(pot_samples))\n",
    "\n",
    "    residue = pot_samples % batch_size\n",
    "    print('\\nresidue is {} '.format(residue))\n",
    "\n",
    "    samples = pot_samples - residue\n",
    "    print('\\nActual samples are {}'.format(samples))\n",
    "\n",
    "    interval = np.arange(0, samples + batch_size, batch_size)\n",
    "    print('\\nPotential intervals: {}'.format(interval ))\n",
    "\n",
    "    interval = np.append(interval, pot_samples)\n",
    "    print('\\nActual interval: {} '.format(interval))\n",
    "\n",
    "    if trim_last_batch:\n",
    "        no_of_batches = len(interval)-2\n",
    "    else:\n",
    "        no_of_batches = len(interval) - 1 \n",
    "\n",
    "    print('\\nNumber of batches are {} '.format(no_of_batches))\n",
    "    \n",
    "    # code for generator\n",
    "    gen_i = 1\n",
    "    while 1:\n",
    "\n",
    "        for b in range(no_of_batches):\n",
    "            st = interval[b]\n",
    "            en = interval[b + 1]\n",
    "            x_batch = x_wins[st:en, :, :]\n",
    "            y_batch = y_wins[st:en]\n",
    "\n",
    "            gen_i +=1\n",
    "            \n",
    "            yield x_batch, y_batch\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_lookback=2  # sequence length\n",
    "input_features = 5\n",
    "output_features = 1\n",
    "_batch_size = 16\n",
    "input_stepsize = 2\n",
    "st_ind = 0\n",
    "end_ind = 600\n",
    "t_plus_ith_val = 1 # which value to predict in future, e.g if input is 11,12,13,14 and default value of this variable means we\n",
    "                  # want to predict 15, setting value equal to 3 means we want to predict 17.\n",
    "\n",
    "\n",
    "train_gen = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                                    input_stepsize, st_ind, end_ind, t_plus_ith_val,\n",
    "                                    norm=True, \n",
    "                                    trim_last_batch = True) \n",
    "\n",
    "val_gen = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                                    input_stepsize,\n",
    "                          min_ind = 600,\n",
    "                          max_ind = 800,\n",
    "                          future_y_val = t_plus_ith_val,\n",
    "                          norm=True,\n",
    "                          trim_last_batch = True) \n",
    "\n",
    "test_gen = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                        input_stepsize,\n",
    "                          min_ind = 800,\n",
    "                          max_ind = 1000,\n",
    "                          future_y_val = t_plus_ith_val,\n",
    "                           norm=True,\n",
    "                          trim_last_batch = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x data: (181, 7, 5) \n",
      "shape of y data: (187, 1)\n",
      ".\n",
      "12 values are skipped from start and 7 values are skipped from end in output array\n",
      "\n",
      "potential samples are 181\n",
      "\n",
      "residue is 5 \n",
      "\n",
      "Actual samples are 176\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 181] \n",
      "\n",
      "Number of batches are 11 \n",
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]\n",
      " [0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]\n",
      " [0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]\n",
      " [0.040201   0.040201   0.040201   0.040201   0.040201  ]\n",
      " [0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]] [0.06532663] \n",
      "\n",
      "[[0.00502513 0.00502513 0.00502513 0.00502513 0.00502513]\n",
      " [0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]\n",
      " [0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]\n",
      " [0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]\n",
      " [0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]\n",
      " [0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]] [0.07035176] \n",
      "\n",
      "[[0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]\n",
      " [0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]\n",
      " [0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]\n",
      " [0.040201   0.040201   0.040201   0.040201   0.040201  ]\n",
      " [0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]] [0.07537688] \n",
      "\n",
      "[[0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]\n",
      " [0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]\n",
      " [0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]\n",
      " [0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]\n",
      " [0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]] [0.08040201] \n",
      "\n",
      "[[0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]\n",
      " [0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]\n",
      " [0.040201   0.040201   0.040201   0.040201   0.040201  ]\n",
      " [0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]] [0.08542714] \n",
      "\n",
      "[[0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]\n",
      " [0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]\n",
      " [0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]\n",
      " [0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]] [0.09045226] \n",
      "\n",
      "[[0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]\n",
      " [0.040201   0.040201   0.040201   0.040201   0.040201  ]\n",
      " [0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]\n",
      " [0.09045226 0.09045226 0.09045226 0.09045226 0.09045226]] [0.09547739] \n",
      "\n",
      "[[0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]\n",
      " [0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]\n",
      " [0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]\n",
      " [0.09547739 0.09547739 0.09547739 0.09547739 0.09547739]] [0.10050251] \n",
      "\n",
      "[[0.040201   0.040201   0.040201   0.040201   0.040201  ]\n",
      " [0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]\n",
      " [0.09045226 0.09045226 0.09045226 0.09045226 0.09045226]\n",
      " [0.10050251 0.10050251 0.10050251 0.10050251 0.10050251]] [0.10552764] \n",
      "\n",
      "[[0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]\n",
      " [0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]\n",
      " [0.09547739 0.09547739 0.09547739 0.09547739 0.09547739]\n",
      " [0.10552764 0.10552764 0.10552764 0.10552764 0.10552764]] [0.11055276] \n",
      "\n",
      "[[0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]\n",
      " [0.09045226 0.09045226 0.09045226 0.09045226 0.09045226]\n",
      " [0.10050251 0.10050251 0.10050251 0.10050251 0.10050251]\n",
      " [0.11055277 0.11055277 0.11055277 0.11055277 0.11055277]] [0.11557789] \n",
      "\n",
      "[[0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]\n",
      " [0.09547739 0.09547739 0.09547739 0.09547739 0.09547739]\n",
      " [0.10552764 0.10552764 0.10552764 0.10552764 0.10552764]\n",
      " [0.11557789 0.11557789 0.11557789 0.11557789 0.11557789]] [0.12060302] \n",
      "\n",
      "[[0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]\n",
      " [0.09045226 0.09045226 0.09045226 0.09045226 0.09045226]\n",
      " [0.10050251 0.10050251 0.10050251 0.10050251 0.10050251]\n",
      " [0.11055277 0.11055277 0.11055277 0.11055277 0.11055277]\n",
      " [0.12060302 0.12060302 0.12060302 0.12060302 0.12060302]] [0.12562814] \n",
      "\n",
      "[[0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]\n",
      " [0.09547739 0.09547739 0.09547739 0.09547739 0.09547739]\n",
      " [0.10552764 0.10552764 0.10552764 0.10552764 0.10552764]\n",
      " [0.11557789 0.11557789 0.11557789 0.11557789 0.11557789]\n",
      " [0.12562814 0.12562814 0.12562814 0.12562814 0.12562814]] [0.13065327] \n",
      "\n",
      "[[0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]\n",
      " [0.09045226 0.09045226 0.09045226 0.09045226 0.09045226]\n",
      " [0.10050251 0.10050251 0.10050251 0.10050251 0.10050251]\n",
      " [0.11055277 0.11055277 0.11055277 0.11055277 0.11055277]\n",
      " [0.12060302 0.12060302 0.12060302 0.12060302 0.12060302]\n",
      " [0.13065326 0.13065326 0.13065326 0.13065326 0.13065326]] [0.13567839] \n",
      "\n",
      "[[0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]\n",
      " [0.09547739 0.09547739 0.09547739 0.09547739 0.09547739]\n",
      " [0.10552764 0.10552764 0.10552764 0.10552764 0.10552764]\n",
      " [0.11557789 0.11557789 0.11557789 0.11557789 0.11557789]\n",
      " [0.12562814 0.12562814 0.12562814 0.12562814 0.12562814]\n",
      " [0.1356784  0.1356784  0.1356784  0.1356784  0.1356784 ]] [0.14070352] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(val_gen)\n",
    "for inp,out in zip(x_batch, y_batch):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x data: (181, 7, 5) \n",
      "shape of y data: (187, 1)\n",
      ".\n",
      "12 values are skipped from start and 7 values are skipped from end in output array\n",
      "\n",
      "potential samples are 181\n",
      "\n",
      "residue is 5 \n",
      "\n",
      "Actual samples are 176\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 181] \n",
      "\n",
      "Number of batches are 11 \n",
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]\n",
      " [0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]\n",
      " [0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]\n",
      " [0.040201   0.040201   0.040201   0.040201   0.040201  ]\n",
      " [0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]] [0.06532663] \n",
      "\n",
      "[[0.00502513 0.00502513 0.00502513 0.00502513 0.00502513]\n",
      " [0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]\n",
      " [0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]\n",
      " [0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]\n",
      " [0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]\n",
      " [0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]] [0.07035176] \n",
      "\n",
      "[[0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]\n",
      " [0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]\n",
      " [0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]\n",
      " [0.040201   0.040201   0.040201   0.040201   0.040201  ]\n",
      " [0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]] [0.07537688] \n",
      "\n",
      "[[0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]\n",
      " [0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]\n",
      " [0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]\n",
      " [0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]\n",
      " [0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]] [0.08040201] \n",
      "\n",
      "[[0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]\n",
      " [0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]\n",
      " [0.040201   0.040201   0.040201   0.040201   0.040201  ]\n",
      " [0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]] [0.08542714] \n",
      "\n",
      "[[0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]\n",
      " [0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]\n",
      " [0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]\n",
      " [0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]] [0.09045226] \n",
      "\n",
      "[[0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]\n",
      " [0.040201   0.040201   0.040201   0.040201   0.040201  ]\n",
      " [0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]\n",
      " [0.09045226 0.09045226 0.09045226 0.09045226 0.09045226]] [0.09547739] \n",
      "\n",
      "[[0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]\n",
      " [0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]\n",
      " [0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]\n",
      " [0.09547739 0.09547739 0.09547739 0.09547739 0.09547739]] [0.10050251] \n",
      "\n",
      "[[0.040201   0.040201   0.040201   0.040201   0.040201  ]\n",
      " [0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]\n",
      " [0.09045226 0.09045226 0.09045226 0.09045226 0.09045226]\n",
      " [0.10050251 0.10050251 0.10050251 0.10050251 0.10050251]] [0.10552764] \n",
      "\n",
      "[[0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]\n",
      " [0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]\n",
      " [0.09547739 0.09547739 0.09547739 0.09547739 0.09547739]\n",
      " [0.10552764 0.10552764 0.10552764 0.10552764 0.10552764]] [0.11055276] \n",
      "\n",
      "[[0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]\n",
      " [0.09045226 0.09045226 0.09045226 0.09045226 0.09045226]\n",
      " [0.10050251 0.10050251 0.10050251 0.10050251 0.10050251]\n",
      " [0.11055277 0.11055277 0.11055277 0.11055277 0.11055277]] [0.11557789] \n",
      "\n",
      "[[0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]\n",
      " [0.09547739 0.09547739 0.09547739 0.09547739 0.09547739]\n",
      " [0.10552764 0.10552764 0.10552764 0.10552764 0.10552764]\n",
      " [0.11557789 0.11557789 0.11557789 0.11557789 0.11557789]] [0.12060302] \n",
      "\n",
      "[[0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]\n",
      " [0.09045226 0.09045226 0.09045226 0.09045226 0.09045226]\n",
      " [0.10050251 0.10050251 0.10050251 0.10050251 0.10050251]\n",
      " [0.11055277 0.11055277 0.11055277 0.11055277 0.11055277]\n",
      " [0.12060302 0.12060302 0.12060302 0.12060302 0.12060302]] [0.12562814] \n",
      "\n",
      "[[0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]\n",
      " [0.09547739 0.09547739 0.09547739 0.09547739 0.09547739]\n",
      " [0.10552764 0.10552764 0.10552764 0.10552764 0.10552764]\n",
      " [0.11557789 0.11557789 0.11557789 0.11557789 0.11557789]\n",
      " [0.12562814 0.12562814 0.12562814 0.12562814 0.12562814]] [0.13065327] \n",
      "\n",
      "[[0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]\n",
      " [0.09045226 0.09045226 0.09045226 0.09045226 0.09045226]\n",
      " [0.10050251 0.10050251 0.10050251 0.10050251 0.10050251]\n",
      " [0.11055277 0.11055277 0.11055277 0.11055277 0.11055277]\n",
      " [0.12060302 0.12060302 0.12060302 0.12060302 0.12060302]\n",
      " [0.13065326 0.13065326 0.13065326 0.13065326 0.13065326]] [0.13567839] \n",
      "\n",
      "[[0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]\n",
      " [0.09547739 0.09547739 0.09547739 0.09547739 0.09547739]\n",
      " [0.10552764 0.10552764 0.10552764 0.10552764 0.10552764]\n",
      " [0.11557789 0.11557789 0.11557789 0.11557789 0.11557789]\n",
      " [0.12562814 0.12562814 0.12562814 0.12562814 0.12562814]\n",
      " [0.1356784  0.1356784  0.1356784  0.1356784  0.1356784 ]] [0.14070352] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(test_gen)\n",
    "for inp,out in zip(x_batch, y_batch):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model using keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "tensorflow.set_random_seed(777)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.0508 - val_loss: 0.0146\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0309 - val_loss: 0.0231\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0239 - val_loss: 0.0567\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0210 - val_loss: 0.0387\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0188 - val_loss: 0.0416\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0179 - val_loss: 0.0337\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0171 - val_loss: 0.0672\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0162 - val_loss: 0.0513\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0158 - val_loss: 0.0600\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0152 - val_loss: 0.0305\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0150 - val_loss: 0.0295\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0139 - val_loss: 0.0494\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0141 - val_loss: 0.0709\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0138 - val_loss: 0.0533\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0134 - val_loss: 0.0744\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0136 - val_loss: 0.0688\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0134 - val_loss: 0.0522\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0129 - val_loss: 0.0398\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0128 - val_loss: 0.0356\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.0126 - val_loss: 0.0282\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.LSTM(64, input_shape=(_lookback, input_features), dropout=0.1, recurrent_dropout=0.5,))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                            steps_per_epoch=500,\n",
    "                            epochs=20,\n",
    "                            validation_data=val_gen,\n",
    "                            validation_steps=195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 10ms/step\n",
      "[5829.] [5827.945]\n",
      "[5830.] [5828.9355]\n",
      "[5831.] [5829.927]\n",
      "[5832.] [5830.918]\n",
      "[5833.] [5831.91]\n",
      "[5834.] [5832.9033]\n",
      "[5835.] [5833.896]\n",
      "[5836.] [5834.8896]\n",
      "[5837.] [5835.8843]\n",
      "[5838.] [5836.879]\n",
      "[5839.] [5837.8745]\n",
      "[5840.] [5838.8706]\n",
      "[5841.] [5839.8677]\n",
      "[5842.] [5840.865]\n",
      "[5843.] [5841.8633]\n",
      "[5844.] [5842.8623]\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y = next(test_gen)\n",
    "test_prediction = model.predict(test_x, 1, verbose=True)\n",
    "test_predict = scaler.inverse_transform(test_prediction)\n",
    "true = scaler.inverse_transform(test_y)\n",
    "for t,p in zip(true, test_predict):\n",
    "    print(t,p)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
