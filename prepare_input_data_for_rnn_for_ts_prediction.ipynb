{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "This notebook describes how to prepare data for RNN/LSTM for timeseries prediction problem especially for Keras/Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Srtqrlwr09Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "np.set_printoptions(suppress=True) # to suppress scientific notation while printing arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data\n",
    "Create a data which is supposed to represent a timeseries prediction problem. The data has 6 columns and 1000 columns. The first five columns are supposed to be input and the last column is supposed to be output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0  2000  4000  6000  8000 10000]\n",
      " [    1  2001  4001  6001  8001 10001]\n",
      " [    2  2002  4002  6002  8002 10002]\n",
      " [    3  2003  4003  6003  8003 10003]\n",
      " [    4  2004  4004  6004  8004 10004]\n",
      " [    5  2005  4005  6005  8005 10005]\n",
      " [    6  2006  4006  6006  8006 10006]\n",
      " [    7  2007  4007  6007  8007 10007]\n",
      " [    8  2008  4008  6008  8008 10008]\n",
      " [    9  2009  4009  6009  8009 10009]\n",
      " [   10  2010  4010  6010  8010 10010]\n",
      " [   11  2011  4011  6011  8011 10011]\n",
      " [   12  2012  4012  6012  8012 10012]\n",
      " [   13  2013  4013  6013  8013 10013]\n",
      " [   14  2014  4014  6014  8014 10014]\n",
      " [   15  2015  4015  6015  8015 10015]\n",
      " [   16  2016  4016  6016  8016 10016]\n",
      " [   17  2017  4017  6017  8017 10017]\n",
      " [   18  2018  4018  6018  8018 10018]\n",
      " [   19  2019  4019  6019  8019 10019]]\n",
      "\n",
      " (2000, 6) \n",
      "\n",
      "[[ 1980  3980  5980  7980  9980 11980]\n",
      " [ 1981  3981  5981  7981  9981 11981]\n",
      " [ 1982  3982  5982  7982  9982 11982]\n",
      " [ 1983  3983  5983  7983  9983 11983]\n",
      " [ 1984  3984  5984  7984  9984 11984]\n",
      " [ 1985  3985  5985  7985  9985 11985]\n",
      " [ 1986  3986  5986  7986  9986 11986]\n",
      " [ 1987  3987  5987  7987  9987 11987]\n",
      " [ 1988  3988  5988  7988  9988 11988]\n",
      " [ 1989  3989  5989  7989  9989 11989]\n",
      " [ 1990  3990  5990  7990  9990 11990]\n",
      " [ 1991  3991  5991  7991  9991 11991]\n",
      " [ 1992  3992  5992  7992  9992 11992]\n",
      " [ 1993  3993  5993  7993  9993 11993]\n",
      " [ 1994  3994  5994  7994  9994 11994]\n",
      " [ 1995  3995  5995  7995  9995 11995]\n",
      " [ 1996  3996  5996  7996  9996 11996]\n",
      " [ 1997  3997  5997  7997  9997 11997]\n",
      " [ 1998  3998  5998  7998  9998 11998]\n",
      " [ 1999  3999  5999  7999  9999 11999]]\n"
     ]
    }
   ],
   "source": [
    "rows = 2000\n",
    "cols = 6\n",
    "data = np.arange(int(rows*cols)).reshape(-1,rows).transpose()\n",
    "print(data[0:20])  \n",
    "print('\\n {} \\n'.format(data.shape))\n",
    "print(data[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x data: (581, 7, 5) \n",
      "shape of y data: (587, 1)\n",
      ".\n",
      "12 values are skipped from start and 7 values are skipped from end in output array\n",
      "\n",
      "potential samples are 581\n",
      "\n",
      "residue is 5 \n",
      "\n",
      "Actual samples are 576\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176 192 208 224 240 256 272\n",
      " 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560\n",
      " 576]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 192 208 224 240 256 272\n",
      " 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560\n",
      " 576 581] \n",
      "\n",
      "Number of batches are 36 \n",
      "\n",
      "shape of batches for:\n",
      "x_data   y_data\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "shape of x data: (181, 7, 5) \n",
      "shape of y data: (187, 1)\n",
      ".\n",
      "12 values are skipped from start and 7 values are skipped from end in output array\n",
      "\n",
      "potential samples are 181\n",
      "\n",
      "residue is 5 \n",
      "\n",
      "Actual samples are 176\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 181] \n",
      "\n",
      "Number of batches are 11 \n",
      "\n",
      "shape of batches for:\n",
      "x_data   y_data\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n"
     ]
    }
   ],
   "source": [
    "def first_nan_from_end(ar):\n",
    "    \"\"\" \n",
    "    This function finds index for first nan from the group which is present at the end of array.\n",
    "    [np.nan, np.nan, 0,2,3,0,3, np.nan, np.nan, np.nan, np.nan] >> 7\n",
    "    [np.nan, np.nan, 1,2,3,0, np.nan, np.nan, np.nan] >> 6\n",
    "    [0,2,3,0,3] >> 5\n",
    "    [np.nan, np.nan, 0,2,3,0,3] >> 7    \n",
    "    \"\"\"\n",
    "    last_non_zero=0\n",
    "    \n",
    "    for idx, val in enumerate(ar[::-1]):\n",
    "        if ~np.isnan(val): # val >= 0:\n",
    "            last_non_zero = idx\n",
    "            break\n",
    "    return ar.shape[0] - last_non_zero    \n",
    "    \n",
    "\n",
    "def batch_generator(data, lookback, in_features, out_features, batch_size, step, min_ind, max_ind, future_y_val,\n",
    "                   norm=None, trim_last_batch=True):\n",
    "    \"\"\"\n",
    "    :param data: `ndarray`, input data.\n",
    "    :param lookback: `int`, sequence length, number of values LSTM will see at time `t` to make prediction at `t+1`.\n",
    "    :in_features: `int`, number of columns in `data` starting from 0 to be considered as input\n",
    "    :out_features: `int`, number of columns in `data` started from last to be considred as output/prediction.\n",
    "    :parm norm: a dictionary which contains scaler object with which to normalize x and y data. We use separate scalers for x\n",
    "                 and y data. Keys must be `x_scaler` and `y_scaler`.\n",
    "    :parm trim_last_batch: bool, if True, last batch will be ignored if that contains samples less than `batch_size`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # selecting the data of interest for x and y    \n",
    "    X = data[min_ind:max_ind, 0:in_features]\n",
    "    Y = data[min_ind:max_ind, -out_features:].reshape(-1,out_features)\n",
    "    \n",
    "    if norm:\n",
    "        x_scaler = norm['x_scaler']\n",
    "        y_scaler = norm['y_scaler']\n",
    "        X = x_scaler.fit_transform(X)\n",
    "        Y = y_scaler.fit_transform(Y)        \n",
    "    \n",
    "    # container for keeping x and y windows. A `windows` is here defined as one complete set of data at one timestep.\n",
    "    x_wins = np.full((X.shape[0], lookback, in_features), np.nan, dtype=np.float32)\n",
    "    y_wins = np.full((Y.shape[0], out_features), np.nan)\n",
    "    \n",
    "    # creating windows from X data\n",
    "    st = lookback*step - step                 # starting point of sampling from data\n",
    "    for j in range(st, X.shape[0]-lookback):\n",
    "        en = j - lookback*step\n",
    "        indices = np.arange(j, en, -step)\n",
    "        ind = np.flip(indices)\n",
    "        x_wins[j,:,:] = X[ind,:]\n",
    "\n",
    "    # creating windows from Y data\n",
    "    for i in range(0, Y.shape[0]-lookback):\n",
    "        y_wins[i,:] = Y[i+lookback,:]\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"removing trailing nans\"\"\"\n",
    "    first_nan_at_end = first_nan_from_end(y_wins[:,0])  # first nan in last part of data, start skipping from here\n",
    "    y_wins = y_wins[0:first_nan_at_end,:]\n",
    "    x_wins = x_wins[0:first_nan_at_end,:]\n",
    "\n",
    "    \"\"\"removing nans from start\"\"\"\n",
    "    y_val = st-lookback + future_y_val\n",
    "    if st>0:\n",
    "        x_wins = x_wins[st:,:]\n",
    "        y_wins = y_wins[y_val:,:]\n",
    "\n",
    "\n",
    "    print(\"\"\"shape of x data: {} \\nshape of y data: {}\"\"\".format(x_wins.shape, y_wins.shape))\n",
    "\n",
    "\n",
    "    print(\"\"\".\\n{} values are skipped from start and {} values are skipped from end in output array\"\"\"\n",
    "          .format(st, X.shape[0]-first_nan_at_end))\n",
    "\n",
    "    pot_samples = x_wins.shape[0]\n",
    "\n",
    "    print('\\npotential samples are {}'.format(pot_samples))\n",
    "\n",
    "    residue = pot_samples % batch_size\n",
    "    print('\\nresidue is {} '.format(residue))\n",
    "\n",
    "    samples = pot_samples - residue\n",
    "    print('\\nActual samples are {}'.format(samples))\n",
    "\n",
    "    interval = np.arange(0, samples + batch_size, batch_size)\n",
    "    print('\\nPotential intervals: {}'.format(interval ))\n",
    "\n",
    "    if residue > 0:\n",
    "        interval = np.append(interval, pot_samples)\n",
    "    print('\\nActual interval: {} '.format(interval))\n",
    "\n",
    "    if trim_last_batch:\n",
    "        no_of_batches = len(interval)-2\n",
    "    else:\n",
    "        no_of_batches = len(interval) - 1 \n",
    "        \n",
    "    print('\\nNumber of batches are {} '.format(no_of_batches))\n",
    "\n",
    "\n",
    "    x_batches = np.full((no_of_batches, batch_size, lookback, in_features), np.nan)\n",
    "    y_batches = np.full((no_of_batches, batch_size, out_features), np.nan)\n",
    "\n",
    "\n",
    "    for b in range(no_of_batches):\n",
    "        st = interval[b]\n",
    "        en = interval[b + 1]\n",
    "        an_x_batch = x_wins[st:en, :, :]\n",
    "        x_batches[b] = an_x_batch\n",
    "       # y_batches[b] = y_wins[st:en]\n",
    "        y_batches[b] = y_wins[st+1:en+1]\n",
    "\n",
    "\n",
    "    print('\\nshape of batches for:')\n",
    "    print('x_data ', ' y_data')\n",
    "    for i,j in zip(x_batches, y_batches):\n",
    "        ishp, jshp = None, None\n",
    "        if isinstance(i, np.ndarray):\n",
    "            ishp = i.shape\n",
    "        if isinstance(j, np.ndarray):\n",
    "            jshp = j.shape\n",
    "        print(ishp, jshp)\n",
    "    \n",
    "    return x_batches, y_batches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_lookback=7  # sequence length\n",
    "input_features = 5\n",
    "output_features = 1\n",
    "_batch_size = 16\n",
    "input_stepsize = 2\n",
    "st_ind = 0\n",
    "end_ind = 600\n",
    "t_plus_ith_val = 1 # which value to predict in future, e.g if input is 11,12,13,14 and default value of this variable means we\n",
    "                  # want to predict 15, setting value equal to 3 means we want to predict 17.\n",
    "\n",
    "train_x_batches, train_y_batches = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                                    input_stepsize, st_ind, end_ind, t_plus_ith_val,\n",
    "                                    trim_last_batch = True)            \n",
    "\n",
    "test_x_batches, test_y_batches = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                                    input_stepsize,\n",
    "                                    min_ind = 600,\n",
    "                                    max_ind = 800,\n",
    "                                    future_y_val = t_plus_ith_val,\n",
    "                                    trim_last_batch = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first train batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0. 2000. 4000. 6000. 8000.]\n",
      " [   2. 2002. 4002. 6002. 8002.]\n",
      " [   4. 2004. 4004. 6004. 8004.]\n",
      " [   6. 2006. 4006. 6006. 8006.]\n",
      " [   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]] [10014.] \n",
      "\n",
      "[[   1. 2001. 4001. 6001. 8001.]\n",
      " [   3. 2003. 4003. 6003. 8003.]\n",
      " [   5. 2005. 4005. 6005. 8005.]\n",
      " [   7. 2007. 4007. 6007. 8007.]\n",
      " [   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]] [10015.] \n",
      "\n",
      "[[   2. 2002. 4002. 6002. 8002.]\n",
      " [   4. 2004. 4004. 6004. 8004.]\n",
      " [   6. 2006. 4006. 6006. 8006.]\n",
      " [   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]] [10016.] \n",
      "\n",
      "[[   3. 2003. 4003. 6003. 8003.]\n",
      " [   5. 2005. 4005. 6005. 8005.]\n",
      " [   7. 2007. 4007. 6007. 8007.]\n",
      " [   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]] [10017.] \n",
      "\n",
      "[[   4. 2004. 4004. 6004. 8004.]\n",
      " [   6. 2006. 4006. 6006. 8006.]\n",
      " [   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]] [10018.] \n",
      "\n",
      "[[   5. 2005. 4005. 6005. 8005.]\n",
      " [   7. 2007. 4007. 6007. 8007.]\n",
      " [   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]] [10019.] \n",
      "\n",
      "[[   6. 2006. 4006. 6006. 8006.]\n",
      " [   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]] [10020.] \n",
      "\n",
      "[[   7. 2007. 4007. 6007. 8007.]\n",
      " [   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]] [10021.] \n",
      "\n",
      "[[   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]] [10022.] \n",
      "\n",
      "[[   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]] [10023.] \n",
      "\n",
      "[[  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]] [10024.] \n",
      "\n",
      "[[  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]] [10025.] \n",
      "\n",
      "[[  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]] [10026.] \n",
      "\n",
      "[[  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]] [10027.] \n",
      "\n",
      "[[  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]] [10028.] \n",
      "\n",
      "[[  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]] [10029.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(train_x_batches[0], train_y_batches[0]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second train batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]] [10030.] \n",
      "\n",
      "[[  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]] [10031.] \n",
      "\n",
      "[[  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]] [10032.] \n",
      "\n",
      "[[  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]] [10033.] \n",
      "\n",
      "[[  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]] [10034.] \n",
      "\n",
      "[[  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]] [10035.] \n",
      "\n",
      "[[  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]] [10036.] \n",
      "\n",
      "[[  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]] [10037.] \n",
      "\n",
      "[[  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]\n",
      " [  36. 2036. 4036. 6036. 8036.]] [10038.] \n",
      "\n",
      "[[  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]\n",
      " [  37. 2037. 4037. 6037. 8037.]] [10039.] \n",
      "\n",
      "[[  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]\n",
      " [  36. 2036. 4036. 6036. 8036.]\n",
      " [  38. 2038. 4038. 6038. 8038.]] [10040.] \n",
      "\n",
      "[[  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]\n",
      " [  37. 2037. 4037. 6037. 8037.]\n",
      " [  39. 2039. 4039. 6039. 8039.]] [10041.] \n",
      "\n",
      "[[  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]\n",
      " [  36. 2036. 4036. 6036. 8036.]\n",
      " [  38. 2038. 4038. 6038. 8038.]\n",
      " [  40. 2040. 4040. 6040. 8040.]] [10042.] \n",
      "\n",
      "[[  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]\n",
      " [  37. 2037. 4037. 6037. 8037.]\n",
      " [  39. 2039. 4039. 6039. 8039.]\n",
      " [  41. 2041. 4041. 6041. 8041.]] [10043.] \n",
      "\n",
      "[[  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]\n",
      " [  36. 2036. 4036. 6036. 8036.]\n",
      " [  38. 2038. 4038. 6038. 8038.]\n",
      " [  40. 2040. 4040. 6040. 8040.]\n",
      " [  42. 2042. 4042. 6042. 8042.]] [10044.] \n",
      "\n",
      "[[  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]\n",
      " [  37. 2037. 4037. 6037. 8037.]\n",
      " [  39. 2039. 4039. 6039. 8039.]\n",
      " [  41. 2041. 4041. 6041. 8041.]\n",
      " [  43. 2043. 4043. 6043. 8043.]] [10045.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(train_x_batches[1], train_y_batches[1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last train batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 560. 2560. 4560. 6560. 8560.]\n",
      " [ 562. 2562. 4562. 6562. 8562.]\n",
      " [ 564. 2564. 4564. 6564. 8564.]\n",
      " [ 566. 2566. 4566. 6566. 8566.]\n",
      " [ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]] [10574.] \n",
      "\n",
      "[[ 561. 2561. 4561. 6561. 8561.]\n",
      " [ 563. 2563. 4563. 6563. 8563.]\n",
      " [ 565. 2565. 4565. 6565. 8565.]\n",
      " [ 567. 2567. 4567. 6567. 8567.]\n",
      " [ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]] [10575.] \n",
      "\n",
      "[[ 562. 2562. 4562. 6562. 8562.]\n",
      " [ 564. 2564. 4564. 6564. 8564.]\n",
      " [ 566. 2566. 4566. 6566. 8566.]\n",
      " [ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]] [10576.] \n",
      "\n",
      "[[ 563. 2563. 4563. 6563. 8563.]\n",
      " [ 565. 2565. 4565. 6565. 8565.]\n",
      " [ 567. 2567. 4567. 6567. 8567.]\n",
      " [ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]] [10577.] \n",
      "\n",
      "[[ 564. 2564. 4564. 6564. 8564.]\n",
      " [ 566. 2566. 4566. 6566. 8566.]\n",
      " [ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]] [10578.] \n",
      "\n",
      "[[ 565. 2565. 4565. 6565. 8565.]\n",
      " [ 567. 2567. 4567. 6567. 8567.]\n",
      " [ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]] [10579.] \n",
      "\n",
      "[[ 566. 2566. 4566. 6566. 8566.]\n",
      " [ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]] [10580.] \n",
      "\n",
      "[[ 567. 2567. 4567. 6567. 8567.]\n",
      " [ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]] [10581.] \n",
      "\n",
      "[[ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]\n",
      " [ 580. 2580. 4580. 6580. 8580.]] [10582.] \n",
      "\n",
      "[[ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]\n",
      " [ 581. 2581. 4581. 6581. 8581.]] [10583.] \n",
      "\n",
      "[[ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]\n",
      " [ 580. 2580. 4580. 6580. 8580.]\n",
      " [ 582. 2582. 4582. 6582. 8582.]] [10584.] \n",
      "\n",
      "[[ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]\n",
      " [ 581. 2581. 4581. 6581. 8581.]\n",
      " [ 583. 2583. 4583. 6583. 8583.]] [10585.] \n",
      "\n",
      "[[ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]\n",
      " [ 580. 2580. 4580. 6580. 8580.]\n",
      " [ 582. 2582. 4582. 6582. 8582.]\n",
      " [ 584. 2584. 4584. 6584. 8584.]] [10586.] \n",
      "\n",
      "[[ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]\n",
      " [ 581. 2581. 4581. 6581. 8581.]\n",
      " [ 583. 2583. 4583. 6583. 8583.]\n",
      " [ 585. 2585. 4585. 6585. 8585.]] [10587.] \n",
      "\n",
      "[[ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]\n",
      " [ 580. 2580. 4580. 6580. 8580.]\n",
      " [ 582. 2582. 4582. 6582. 8582.]\n",
      " [ 584. 2584. 4584. 6584. 8584.]\n",
      " [ 586. 2586. 4586. 6586. 8586.]] [10588.] \n",
      "\n",
      "[[ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]\n",
      " [ 581. 2581. 4581. 6581. 8581.]\n",
      " [ 583. 2583. 4583. 6583. 8583.]\n",
      " [ 585. 2585. 4585. 6585. 8585.]\n",
      " [ 587. 2587. 4587. 6587. 8587.]] [10589.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(train_x_batches[-1], train_y_batches[-1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 600. 2600. 4600. 6600. 8600.]\n",
      " [ 602. 2602. 4602. 6602. 8602.]\n",
      " [ 604. 2604. 4604. 6604. 8604.]\n",
      " [ 606. 2606. 4606. 6606. 8606.]\n",
      " [ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]] [10614.] \n",
      "\n",
      "[[ 601. 2601. 4601. 6601. 8601.]\n",
      " [ 603. 2603. 4603. 6603. 8603.]\n",
      " [ 605. 2605. 4605. 6605. 8605.]\n",
      " [ 607. 2607. 4607. 6607. 8607.]\n",
      " [ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]] [10615.] \n",
      "\n",
      "[[ 602. 2602. 4602. 6602. 8602.]\n",
      " [ 604. 2604. 4604. 6604. 8604.]\n",
      " [ 606. 2606. 4606. 6606. 8606.]\n",
      " [ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]] [10616.] \n",
      "\n",
      "[[ 603. 2603. 4603. 6603. 8603.]\n",
      " [ 605. 2605. 4605. 6605. 8605.]\n",
      " [ 607. 2607. 4607. 6607. 8607.]\n",
      " [ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]] [10617.] \n",
      "\n",
      "[[ 604. 2604. 4604. 6604. 8604.]\n",
      " [ 606. 2606. 4606. 6606. 8606.]\n",
      " [ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]] [10618.] \n",
      "\n",
      "[[ 605. 2605. 4605. 6605. 8605.]\n",
      " [ 607. 2607. 4607. 6607. 8607.]\n",
      " [ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]] [10619.] \n",
      "\n",
      "[[ 606. 2606. 4606. 6606. 8606.]\n",
      " [ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]] [10620.] \n",
      "\n",
      "[[ 607. 2607. 4607. 6607. 8607.]\n",
      " [ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]] [10621.] \n",
      "\n",
      "[[ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]] [10622.] \n",
      "\n",
      "[[ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]] [10623.] \n",
      "\n",
      "[[ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]] [10624.] \n",
      "\n",
      "[[ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]] [10625.] \n",
      "\n",
      "[[ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]] [10626.] \n",
      "\n",
      "[[ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]] [10627.] \n",
      "\n",
      "[[ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]] [10628.] \n",
      "\n",
      "[[ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]] [10629.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[0], test_y_batches[0]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]] [10630.] \n",
      "\n",
      "[[ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]] [10631.] \n",
      "\n",
      "[[ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]] [10632.] \n",
      "\n",
      "[[ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]] [10633.] \n",
      "\n",
      "[[ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]] [10634.] \n",
      "\n",
      "[[ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]] [10635.] \n",
      "\n",
      "[[ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]] [10636.] \n",
      "\n",
      "[[ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]] [10637.] \n",
      "\n",
      "[[ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]\n",
      " [ 636. 2636. 4636. 6636. 8636.]] [10638.] \n",
      "\n",
      "[[ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]\n",
      " [ 637. 2637. 4637. 6637. 8637.]] [10639.] \n",
      "\n",
      "[[ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]\n",
      " [ 636. 2636. 4636. 6636. 8636.]\n",
      " [ 638. 2638. 4638. 6638. 8638.]] [10640.] \n",
      "\n",
      "[[ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]\n",
      " [ 637. 2637. 4637. 6637. 8637.]\n",
      " [ 639. 2639. 4639. 6639. 8639.]] [10641.] \n",
      "\n",
      "[[ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]\n",
      " [ 636. 2636. 4636. 6636. 8636.]\n",
      " [ 638. 2638. 4638. 6638. 8638.]\n",
      " [ 640. 2640. 4640. 6640. 8640.]] [10642.] \n",
      "\n",
      "[[ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]\n",
      " [ 637. 2637. 4637. 6637. 8637.]\n",
      " [ 639. 2639. 4639. 6639. 8639.]\n",
      " [ 641. 2641. 4641. 6641. 8641.]] [10643.] \n",
      "\n",
      "[[ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]\n",
      " [ 636. 2636. 4636. 6636. 8636.]\n",
      " [ 638. 2638. 4638. 6638. 8638.]\n",
      " [ 640. 2640. 4640. 6640. 8640.]\n",
      " [ 642. 2642. 4642. 6642. 8642.]] [10644.] \n",
      "\n",
      "[[ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]\n",
      " [ 637. 2637. 4637. 6637. 8637.]\n",
      " [ 639. 2639. 4639. 6639. 8639.]\n",
      " [ 641. 2641. 4641. 6641. 8641.]\n",
      " [ 643. 2643. 4643. 6643. 8643.]] [10645.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[1], test_y_batches[1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second last test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 744. 2744. 4744. 6744. 8744.]\n",
      " [ 746. 2746. 4746. 6746. 8746.]\n",
      " [ 748. 2748. 4748. 6748. 8748.]\n",
      " [ 750. 2750. 4750. 6750. 8750.]\n",
      " [ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]] [10758.] \n",
      "\n",
      "[[ 745. 2745. 4745. 6745. 8745.]\n",
      " [ 747. 2747. 4747. 6747. 8747.]\n",
      " [ 749. 2749. 4749. 6749. 8749.]\n",
      " [ 751. 2751. 4751. 6751. 8751.]\n",
      " [ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]] [10759.] \n",
      "\n",
      "[[ 746. 2746. 4746. 6746. 8746.]\n",
      " [ 748. 2748. 4748. 6748. 8748.]\n",
      " [ 750. 2750. 4750. 6750. 8750.]\n",
      " [ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]] [10760.] \n",
      "\n",
      "[[ 747. 2747. 4747. 6747. 8747.]\n",
      " [ 749. 2749. 4749. 6749. 8749.]\n",
      " [ 751. 2751. 4751. 6751. 8751.]\n",
      " [ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]] [10761.] \n",
      "\n",
      "[[ 748. 2748. 4748. 6748. 8748.]\n",
      " [ 750. 2750. 4750. 6750. 8750.]\n",
      " [ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]] [10762.] \n",
      "\n",
      "[[ 749. 2749. 4749. 6749. 8749.]\n",
      " [ 751. 2751. 4751. 6751. 8751.]\n",
      " [ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]] [10763.] \n",
      "\n",
      "[[ 750. 2750. 4750. 6750. 8750.]\n",
      " [ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]] [10764.] \n",
      "\n",
      "[[ 751. 2751. 4751. 6751. 8751.]\n",
      " [ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]] [10765.] \n",
      "\n",
      "[[ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]] [10766.] \n",
      "\n",
      "[[ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]] [10767.] \n",
      "\n",
      "[[ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]] [10768.] \n",
      "\n",
      "[[ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]] [10769.] \n",
      "\n",
      "[[ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]] [10770.] \n",
      "\n",
      "[[ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]] [10771.] \n",
      "\n",
      "[[ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]] [10772.] \n",
      "\n",
      "[[ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]] [10773.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[-2], test_y_batches[-2]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]] [10774.] \n",
      "\n",
      "[[ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]] [10775.] \n",
      "\n",
      "[[ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]] [10776.] \n",
      "\n",
      "[[ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]] [10777.] \n",
      "\n",
      "[[ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]] [10778.] \n",
      "\n",
      "[[ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]] [10779.] \n",
      "\n",
      "[[ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]] [10780.] \n",
      "\n",
      "[[ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]] [10781.] \n",
      "\n",
      "[[ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]\n",
      " [ 780. 2780. 4780. 6780. 8780.]] [10782.] \n",
      "\n",
      "[[ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]\n",
      " [ 781. 2781. 4781. 6781. 8781.]] [10783.] \n",
      "\n",
      "[[ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]\n",
      " [ 780. 2780. 4780. 6780. 8780.]\n",
      " [ 782. 2782. 4782. 6782. 8782.]] [10784.] \n",
      "\n",
      "[[ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]\n",
      " [ 781. 2781. 4781. 6781. 8781.]\n",
      " [ 783. 2783. 4783. 6783. 8783.]] [10785.] \n",
      "\n",
      "[[ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]\n",
      " [ 780. 2780. 4780. 6780. 8780.]\n",
      " [ 782. 2782. 4782. 6782. 8782.]\n",
      " [ 784. 2784. 4784. 6784. 8784.]] [10786.] \n",
      "\n",
      "[[ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]\n",
      " [ 781. 2781. 4781. 6781. 8781.]\n",
      " [ 783. 2783. 4783. 6783. 8783.]\n",
      " [ 785. 2785. 4785. 6785. 8785.]] [10787.] \n",
      "\n",
      "[[ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]\n",
      " [ 780. 2780. 4780. 6780. 8780.]\n",
      " [ 782. 2782. 4782. 6782. 8782.]\n",
      " [ 784. 2784. 4784. 6784. 8784.]\n",
      " [ 786. 2786. 4786. 6786. 8786.]] [10788.] \n",
      "\n",
      "[[ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]\n",
      " [ 781. 2781. 4781. 6781. 8781.]\n",
      " [ 783. 2783. 4783. 6783. 8783.]\n",
      " [ 785. 2785. 4785. 6785. 8785.]\n",
      " [ 787. 2787. 4787. 6787. 8787.]] [10789.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[-1], test_y_batches[-1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Generator using `yield`\n",
    "Instead of using `return` statement, we can use `yield` statement, which is more memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train_x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "val_x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "val_y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "test_x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "test_y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    " \n",
    "\n",
    "def batch_generator(data, lookback, in_features, out_features, batch_size, min_ind, max_ind, future_y_val,\n",
    "                   step =1, norm=None, trim_last_batch=True):\n",
    "    \"\"\"\n",
    "    :param data: `ndarray`, input data.\n",
    "    :param lookback: `int`, sequence length, number of values LSTM will see at time `t` to make prediction at `t+1`.\n",
    "    :param in_features: `int`, number of columns in `data` starting from 0 to be considered as input\n",
    "    :param out_features: `int`, number of columns in `data` started from last to be considred as output/prediction.\n",
    "    :param trim_last_batch: bool, if True, last batch will be ignored if that contains samples less than `batch_size`.\n",
    "    :param norm: a dictionary which contains scaler object with which to normalize x and y data. We use separate scalers for x\n",
    "                 and y data. Keys must be `x_scaler` and `y_scaler`.\n",
    "    :param batch_size:\n",
    "    :param step: step size in input data\n",
    "    :param min_ind: starting point from `data`\n",
    "    :param max_ind: end point from `data`\n",
    "    :param future_y_val: number of values to predict\n",
    "    \"\"\"\n",
    "\n",
    "    # selecting the data of interest for x and y    \n",
    "    X = data[min_ind:max_ind, 0:in_features]\n",
    "    Y = data[min_ind:max_ind, -out_features:].reshape(-1,out_features)\n",
    "    \n",
    "    if norm is not None:\n",
    "        x_scaler = norm['x_scaler']\n",
    "        y_scaler = norm['y_scaler']\n",
    "        X = x_scaler.fit_transform(X)\n",
    "        Y = y_scaler.fit_transform(Y)\n",
    "    \n",
    "    # container for keeping x and y windows. A `windows` is here defined as one complete set of data at one timestep.\n",
    "    x_wins = np.full((X.shape[0], lookback, in_features), np.nan, dtype=np.float32)\n",
    "    y_wins = np.full((Y.shape[0], out_features), np.nan)\n",
    "\n",
    "    # creating windows from X data\n",
    "    st = lookback*step - step # starting point of sampling from data\n",
    "    for j in range(st, X.shape[0]-lookback):\n",
    "        en = j - lookback*step\n",
    "        indices = np.arange(j, en, -step)\n",
    "        ind = np.flip(indices)\n",
    "        x_wins[j,:,:] = X[ind,:]\n",
    "\n",
    "    # creating windows from Y data\n",
    "    for i in range(0, Y.shape[0]-lookback):\n",
    "        y_wins[i,:] = Y[i+lookback,:]\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"removing trailing nans\"\"\"\n",
    "    first_nan_at_end = first_nan_from_end(y_wins[:,0])  # first nan in last part of data, start skipping from here\n",
    "    y_wins = y_wins[0:first_nan_at_end,:]\n",
    "    x_wins = x_wins[0:first_nan_at_end,:]\n",
    "\n",
    "    \"\"\"removing nans from start\"\"\"\n",
    "    y_val = st-lookback + future_y_val\n",
    "    if st>0:\n",
    "        x_wins = x_wins[st:,:]\n",
    "        y_wins = y_wins[y_val:,:]    \n",
    "\n",
    "    print(\"\"\"shape of x data: {} \\nshape of y data: {}\"\"\".format(x_wins.shape, y_wins.shape))\n",
    "\n",
    "    print(\"\"\".\\n{} values are skipped from start and {} values are skipped from end in output array\"\"\"\n",
    "          .format(st, X.shape[0]-first_nan_at_end))\n",
    "\n",
    "    pot_samples = x_wins.shape[0]\n",
    "\n",
    "    print('\\npotential samples are {}'.format(pot_samples))\n",
    "\n",
    "    residue = pot_samples % batch_size\n",
    "    print('\\nresidue is {} '.format(residue))\n",
    "\n",
    "    samples = pot_samples - residue\n",
    "    print('\\nActual samples are {}'.format(samples))\n",
    "\n",
    "    interval = np.arange(0, samples + batch_size, batch_size)\n",
    "    print('\\nPotential intervals: {}'.format(interval ))\n",
    "\n",
    "    interval = np.append(interval, pot_samples)\n",
    "    print('\\nActual interval: {} '.format(interval))\n",
    "\n",
    "    if trim_last_batch:\n",
    "        no_of_batches = len(interval)-2\n",
    "    else:\n",
    "        no_of_batches = len(interval) - 1 \n",
    "\n",
    "    print('\\nNumber of batches are {} '.format(no_of_batches))\n",
    "    \n",
    "    # code for generator\n",
    "    gen_i = 1\n",
    "    while 1:\n",
    "\n",
    "        for b in range(no_of_batches):\n",
    "            st = interval[b]\n",
    "            en = interval[b + 1]\n",
    "            x_batch = x_wins[st:en, :, :]\n",
    "            y_batch = y_wins[st:en]\n",
    "\n",
    "            gen_i +=1\n",
    "            \n",
    "            yield x_batch, y_batch\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_lookback=2  # sequence length\n",
    "input_features = 5\n",
    "output_features = 1\n",
    "_batch_size = 16\n",
    "input_stepsize = 2\n",
    "st_ind = 0\n",
    "end_ind = 600\n",
    "t_plus_ith_val = 1 # which value to predict in future, e.g if input is 11,12,13,14 and default value of this variable means we\n",
    "                  # want to predict 15, setting value equal to 3 means we want to predict 17.\n",
    "\n",
    "\n",
    "train_gen = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                            st_ind,\n",
    "                            end_ind,\n",
    "                            t_plus_ith_val,\n",
    "                            step=input_stepsize,\n",
    "                            norm={'x_scaler': train_x_scaler, 'y_scaler': train_y_scaler},\n",
    "                            trim_last_batch = True) \n",
    "\n",
    "val_gen = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                          min_ind = 600,\n",
    "                          max_ind = 800,\n",
    "                          future_y_val = t_plus_ith_val,\n",
    "                          step=input_stepsize,\n",
    "                          norm={'x_scaler': val_x_scaler, 'y_scaler': val_y_scaler},\n",
    "                          trim_last_batch = True) \n",
    "\n",
    "test_gen = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                          min_ind = 800,\n",
    "                          max_ind = 1000,\n",
    "                          future_y_val = t_plus_ith_val,\n",
    "                          step=input_stepsize,\n",
    "                           norm={'x_scaler': test_x_scaler, 'y_scaler': test_y_scaler},\n",
    "                          trim_last_batch = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x data: (196, 2, 5) \n",
      "shape of y data: (197, 1)\n",
      ".\n",
      "2 values are skipped from start and 2 values are skipped from end in output array\n",
      "\n",
      "potential samples are 196\n",
      "\n",
      "residue is 4 \n",
      "\n",
      "Actual samples are 192\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176 192]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 192 196] \n",
      "\n",
      "Number of batches are 12 \n",
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]] [0.01507538] \n",
      "\n",
      "[[0.00502513 0.00502513 0.00502513 0.00502513 0.00502513]\n",
      " [0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]] [0.0201005] \n",
      "\n",
      "[[0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]\n",
      " [0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]] [0.02512563] \n",
      "\n",
      "[[0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]\n",
      " [0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]] [0.03015075] \n",
      "\n",
      "[[0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]\n",
      " [0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]] [0.03517588] \n",
      "\n",
      "[[0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]\n",
      " [0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]] [0.04020101] \n",
      "\n",
      "[[0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]\n",
      " [0.040201   0.040201   0.040201   0.040201   0.040201  ]] [0.04522613] \n",
      "\n",
      "[[0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]\n",
      " [0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]] [0.05025126] \n",
      "\n",
      "[[0.040201   0.040201   0.040201   0.040201   0.040201  ]\n",
      " [0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]] [0.05527638] \n",
      "\n",
      "[[0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]\n",
      " [0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]] [0.06030151] \n",
      "\n",
      "[[0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]] [0.06532663] \n",
      "\n",
      "[[0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]] [0.07035176] \n",
      "\n",
      "[[0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]] [0.07537688] \n",
      "\n",
      "[[0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]] [0.08040201] \n",
      "\n",
      "[[0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]] [0.08542714] \n",
      "\n",
      "[[0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]] [0.09045226] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(val_gen)\n",
    "for inp,out in zip(x_batch, y_batch):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x data: (196, 2, 5) \n",
      "shape of y data: (197, 1)\n",
      ".\n",
      "2 values are skipped from start and 2 values are skipped from end in output array\n",
      "\n",
      "potential samples are 196\n",
      "\n",
      "residue is 4 \n",
      "\n",
      "Actual samples are 192\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176 192]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 192 196] \n",
      "\n",
      "Number of batches are 12 \n",
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]] [0.01507538] \n",
      "\n",
      "[[0.00502513 0.00502513 0.00502513 0.00502513 0.00502513]\n",
      " [0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]] [0.0201005] \n",
      "\n",
      "[[0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]\n",
      " [0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]] [0.02512563] \n",
      "\n",
      "[[0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]\n",
      " [0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]] [0.03015075] \n",
      "\n",
      "[[0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]\n",
      " [0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]] [0.03517588] \n",
      "\n",
      "[[0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]\n",
      " [0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]] [0.04020101] \n",
      "\n",
      "[[0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]\n",
      " [0.040201   0.040201   0.040201   0.040201   0.040201  ]] [0.04522613] \n",
      "\n",
      "[[0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]\n",
      " [0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]] [0.05025126] \n",
      "\n",
      "[[0.040201   0.040201   0.040201   0.040201   0.040201  ]\n",
      " [0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]] [0.05527638] \n",
      "\n",
      "[[0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]\n",
      " [0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]] [0.06030151] \n",
      "\n",
      "[[0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]] [0.06532663] \n",
      "\n",
      "[[0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]] [0.07035176] \n",
      "\n",
      "[[0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]] [0.07537688] \n",
      "\n",
      "[[0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]] [0.08040201] \n",
      "\n",
      "[[0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]] [0.08542714] \n",
      "\n",
      "[[0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]] [0.09045226] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(test_gen)\n",
    "for inp,out in zip(x_batch, y_batch):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model using keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "tensorflow.set_random_seed(777)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1216 11:15:20.042056 147620 deprecation.py:506] From C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "W1216 11:15:20.565654 147620 deprecation.py:323] From C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "shape of x data: (596, 2, 5) \n",
      "shape of y data: (597, 1)\n",
      ".\n",
      "2 values are skipped from start and 2 values are skipped from end in output array\n",
      "\n",
      "potential samples are 596\n",
      "\n",
      "residue is 4 \n",
      "\n",
      "Actual samples are 592\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176 192 208 224 240 256 272\n",
      " 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560\n",
      " 576 592]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 192 208 224 240 256 272\n",
      " 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560\n",
      " 576 592 596] \n",
      "\n",
      "Number of batches are 37 \n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.0524 - val_loss: 0.0566\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0473 - val_loss: 0.0190\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0377 - val_loss: 0.0345\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0299 - val_loss: 0.0106\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0275 - val_loss: 0.0345\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0266 - val_loss: 0.0132\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0240 - val_loss: 0.0597\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0226 - val_loss: 0.0326\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0197 - val_loss: 0.0528\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0188 - val_loss: 0.0193\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0170 - val_loss: 0.0286\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0166 - val_loss: 0.0273\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0153 - val_loss: 0.0595\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0153 - val_loss: 0.0318\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0148 - val_loss: 0.0499\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0145 - val_loss: 0.0152\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0139 - val_loss: 0.0427\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0138 - val_loss: 0.0384\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.0136 - val_loss: 0.0343\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.0137 - val_loss: 0.0320\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.LSTM(64, input_shape=(_lookback, input_features), dropout=0.1, recurrent_dropout=0.5,))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                            steps_per_epoch=500,\n",
    "                            epochs=20,\n",
    "                            validation_data=val_gen,\n",
    "                            validation_steps=195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 8ms/sample\n",
      "[10819.] [10818.187]\n",
      "[10820.] [10819.045]\n",
      "[10821.] [10819.906]\n",
      "[10822.] [10820.771]\n",
      "[10823.] [10821.64]\n",
      "[10824.] [10822.512]\n",
      "[10825.] [10823.387]\n",
      "[10826.] [10824.266]\n",
      "[10827.] [10825.147]\n",
      "[10828.] [10826.031]\n",
      "[10829.] [10826.92]\n",
      "[10830.] [10827.811]\n",
      "[10831.] [10828.704]\n",
      "[10832.] [10829.601]\n",
      "[10833.] [10830.5]\n",
      "[10834.] [10831.403]\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y = next(test_gen)\n",
    "test_prediction = model.predict(test_x, 1, verbose=True)\n",
    "test_predict = test_y_scaler.inverse_transform(test_prediction)\n",
    "true = test_y_scaler.inverse_transform(test_y)\n",
    "for t,p in zip(true, test_predict):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
