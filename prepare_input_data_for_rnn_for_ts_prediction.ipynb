{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "This notebook describes how to prepare data for RNN/LSTM for timeseries prediction problem especially for Keras/Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Srtqrlwr09Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "np.set_printoptions(suppress=True) # to suppress scientific notation while printing arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data\n",
    "Create a data which is supposed to represent a timeseries prediction problem. The data has 6 columns and 1000 columns. The first five columns are supposed to be input and the last column is supposed to be output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0  2000  4000  6000  8000 10000]\n",
      " [    1  2001  4001  6001  8001 10001]\n",
      " [    2  2002  4002  6002  8002 10002]\n",
      " [    3  2003  4003  6003  8003 10003]\n",
      " [    4  2004  4004  6004  8004 10004]\n",
      " [    5  2005  4005  6005  8005 10005]\n",
      " [    6  2006  4006  6006  8006 10006]\n",
      " [    7  2007  4007  6007  8007 10007]\n",
      " [    8  2008  4008  6008  8008 10008]\n",
      " [    9  2009  4009  6009  8009 10009]\n",
      " [   10  2010  4010  6010  8010 10010]\n",
      " [   11  2011  4011  6011  8011 10011]\n",
      " [   12  2012  4012  6012  8012 10012]\n",
      " [   13  2013  4013  6013  8013 10013]\n",
      " [   14  2014  4014  6014  8014 10014]\n",
      " [   15  2015  4015  6015  8015 10015]\n",
      " [   16  2016  4016  6016  8016 10016]\n",
      " [   17  2017  4017  6017  8017 10017]\n",
      " [   18  2018  4018  6018  8018 10018]\n",
      " [   19  2019  4019  6019  8019 10019]]\n",
      "\n",
      " (2000, 6) \n",
      "\n",
      "[[ 1980  3980  5980  7980  9980 11980]\n",
      " [ 1981  3981  5981  7981  9981 11981]\n",
      " [ 1982  3982  5982  7982  9982 11982]\n",
      " [ 1983  3983  5983  7983  9983 11983]\n",
      " [ 1984  3984  5984  7984  9984 11984]\n",
      " [ 1985  3985  5985  7985  9985 11985]\n",
      " [ 1986  3986  5986  7986  9986 11986]\n",
      " [ 1987  3987  5987  7987  9987 11987]\n",
      " [ 1988  3988  5988  7988  9988 11988]\n",
      " [ 1989  3989  5989  7989  9989 11989]\n",
      " [ 1990  3990  5990  7990  9990 11990]\n",
      " [ 1991  3991  5991  7991  9991 11991]\n",
      " [ 1992  3992  5992  7992  9992 11992]\n",
      " [ 1993  3993  5993  7993  9993 11993]\n",
      " [ 1994  3994  5994  7994  9994 11994]\n",
      " [ 1995  3995  5995  7995  9995 11995]\n",
      " [ 1996  3996  5996  7996  9996 11996]\n",
      " [ 1997  3997  5997  7997  9997 11997]\n",
      " [ 1998  3998  5998  7998  9998 11998]\n",
      " [ 1999  3999  5999  7999  9999 11999]]\n"
     ]
    }
   ],
   "source": [
    "rows = 2000\n",
    "cols = 6\n",
    "data = np.arange(int(rows*cols)).reshape(-1,rows).transpose()\n",
    "print(data[0:20])  \n",
    "print('\\n {} \\n'.format(data.shape))\n",
    "print(data[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x data: (581, 7, 5) \n",
      "shape of y data: (587, 1)\n",
      ".\n",
      "12 values are skipped from start and 7 values are skipped from end in output array\n",
      "\n",
      "potential samples are 581\n",
      "\n",
      "residue is 5 \n",
      "\n",
      "Actual samples are 576\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176 192 208 224 240 256 272\n",
      " 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560\n",
      " 576]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 192 208 224 240 256 272\n",
      " 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560\n",
      " 576 581] \n",
      "\n",
      "Number of batches are 36 \n",
      "\n",
      "shape of batches for:\n",
      "x_data   y_data\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "shape of x data: (181, 7, 5) \n",
      "shape of y data: (187, 1)\n",
      ".\n",
      "12 values are skipped from start and 7 values are skipped from end in output array\n",
      "\n",
      "potential samples are 181\n",
      "\n",
      "residue is 5 \n",
      "\n",
      "Actual samples are 176\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 181] \n",
      "\n",
      "Number of batches are 11 \n",
      "\n",
      "shape of batches for:\n",
      "x_data   y_data\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n",
      "(16, 7, 5) (16, 1)\n"
     ]
    }
   ],
   "source": [
    "def first_nan_from_end(ar):\n",
    "    \"\"\" \n",
    "    This function finds index for first nan from the group which is present at the end of array.\n",
    "    [np.nan, np.nan, 0,2,3,0,3, np.nan, np.nan, np.nan, np.nan] >> 7\n",
    "    [np.nan, np.nan, 1,2,3,0, np.nan, np.nan, np.nan] >> 6\n",
    "    [0,2,3,0,3] >> 5\n",
    "    [np.nan, np.nan, 0,2,3,0,3] >> 7    \n",
    "    \"\"\"\n",
    "    last_non_zero=0\n",
    "    \n",
    "    for idx, val in enumerate(ar[::-1]):\n",
    "        if ~np.isnan(val): # val >= 0:\n",
    "            last_non_zero = idx\n",
    "            break\n",
    "    return ar.shape[0] - last_non_zero    \n",
    "    \n",
    "\n",
    "def batch_generator(data, lookback, in_features, out_features, batch_size, step, min_ind, max_ind, future_y_val,\n",
    "                   norm=None, trim_last_batch=True):\n",
    "    \"\"\"\n",
    "    :param data: `ndarray`, input data.\n",
    "    :param lookback: `int`, sequence length, number of values LSTM will see at time `t` to make prediction at `t+1`.\n",
    "    :in_features: `int`, number of columns in `data` starting from 0 to be considered as input\n",
    "    :out_features: `int`, number of columns in `data` started from last to be considred as output/prediction.\n",
    "    :parm norm: a dictionary which contains scaler object with which to normalize x and y data. We use separate scalers for x\n",
    "                 and y data. Keys must be `x_scaler` and `y_scaler`.\n",
    "    :parm trim_last_batch: bool, if True, last batch will be ignored if that contains samples less than `batch_size`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # selecting the data of interest for x and y    \n",
    "    X = data[min_ind:max_ind, 0:in_features]\n",
    "    Y = data[min_ind:max_ind, -out_features:].reshape(-1,out_features)\n",
    "    \n",
    "    if norm:\n",
    "        x_scaler = norm['x_scaler']\n",
    "        y_scaler = norm['y_scaler']\n",
    "        X = x_scaler.fit_transform(X)\n",
    "        Y = y_scaler.fit_transform(Y)        \n",
    "    \n",
    "    # container for keeping x and y windows. A `windows` is here defined as one complete set of data at one timestep.\n",
    "    x_wins = np.full((X.shape[0], lookback, in_features), np.nan, dtype=np.float32)\n",
    "    y_wins = np.full((Y.shape[0], out_features), np.nan)\n",
    "    \n",
    "    # creating windows from X data\n",
    "    st = lookback*step - step                 # starting point of sampling from data\n",
    "    for j in range(st, X.shape[0]-lookback):\n",
    "        en = j - lookback*step\n",
    "        indices = np.arange(j, en, -step)\n",
    "        ind = np.flip(indices)\n",
    "        x_wins[j,:,:] = X[ind,:]\n",
    "\n",
    "    # creating windows from Y data\n",
    "    for i in range(0, Y.shape[0]-lookback):\n",
    "        y_wins[i,:] = Y[i+lookback,:]\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"removing trailing nans\"\"\"\n",
    "    first_nan_at_end = first_nan_from_end(y_wins[:,0])  # first nan in last part of data, start skipping from here\n",
    "    y_wins = y_wins[0:first_nan_at_end,:]\n",
    "    x_wins = x_wins[0:first_nan_at_end,:]\n",
    "\n",
    "    \"\"\"removing nans from start\"\"\"\n",
    "    y_val = st-lookback + future_y_val\n",
    "    if st>0:\n",
    "        x_wins = x_wins[st:,:]\n",
    "        y_wins = y_wins[y_val:,:]\n",
    "\n",
    "\n",
    "    print(\"\"\"shape of x data: {} \\nshape of y data: {}\"\"\".format(x_wins.shape, y_wins.shape))\n",
    "\n",
    "\n",
    "    print(\"\"\".\\n{} values are skipped from start and {} values are skipped from end in output array\"\"\"\n",
    "          .format(st, X.shape[0]-first_nan_at_end))\n",
    "\n",
    "    pot_samples = x_wins.shape[0]\n",
    "\n",
    "    print('\\npotential samples are {}'.format(pot_samples))\n",
    "\n",
    "    residue = pot_samples % batch_size\n",
    "    print('\\nresidue is {} '.format(residue))\n",
    "\n",
    "    samples = pot_samples - residue\n",
    "    print('\\nActual samples are {}'.format(samples))\n",
    "\n",
    "    interval = np.arange(0, samples + batch_size, batch_size)\n",
    "    print('\\nPotential intervals: {}'.format(interval ))\n",
    "\n",
    "    if residue > 0:\n",
    "        interval = np.append(interval, pot_samples)\n",
    "    print('\\nActual interval: {} '.format(interval))\n",
    "\n",
    "    if trim_last_batch:\n",
    "        no_of_batches = len(interval)-2\n",
    "    else:\n",
    "        no_of_batches = len(interval) - 1 \n",
    "        \n",
    "    print('\\nNumber of batches are {} '.format(no_of_batches))\n",
    "\n",
    "\n",
    "    x_batches = np.full((no_of_batches, batch_size, lookback, in_features), np.nan)\n",
    "    y_batches = np.full((no_of_batches, batch_size, out_features), np.nan)\n",
    "\n",
    "\n",
    "    for b in range(no_of_batches):\n",
    "        st = interval[b]\n",
    "        en = interval[b + 1]\n",
    "        an_x_batch = x_wins[st:en, :, :]\n",
    "        x_batches[b] = an_x_batch\n",
    "       # y_batches[b] = y_wins[st:en]\n",
    "        y_batches[b] = y_wins[st+1:en+1]\n",
    "\n",
    "\n",
    "    print('\\nshape of batches for:')\n",
    "    print('x_data ', ' y_data')\n",
    "    for i,j in zip(x_batches, y_batches):\n",
    "        ishp, jshp = None, None\n",
    "        if isinstance(i, np.ndarray):\n",
    "            ishp = i.shape\n",
    "        if isinstance(j, np.ndarray):\n",
    "            jshp = j.shape\n",
    "        print(ishp, jshp)\n",
    "    \n",
    "    return x_batches, y_batches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_lookback=7  # sequence length\n",
    "input_features = 5\n",
    "output_features = 1\n",
    "_batch_size = 16\n",
    "input_stepsize = 2\n",
    "st_ind = 0\n",
    "end_ind = 600\n",
    "t_plus_ith_val = 1 # which value to predict in future, e.g if input is 11,12,13,14 and default value of this variable means we\n",
    "                  # want to predict 15, setting value equal to 3 means we want to predict 17.\n",
    "\n",
    "train_x_batches, train_y_batches = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                                    input_stepsize, st_ind, end_ind, t_plus_ith_val,\n",
    "                                    trim_last_batch = True)            \n",
    "\n",
    "test_x_batches, test_y_batches = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                                    input_stepsize,\n",
    "                                    min_ind = 600,\n",
    "                                    max_ind = 800,\n",
    "                                    future_y_val = t_plus_ith_val,\n",
    "                                    trim_last_batch = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first train batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0. 2000. 4000. 6000. 8000.]\n",
      " [   2. 2002. 4002. 6002. 8002.]\n",
      " [   4. 2004. 4004. 6004. 8004.]\n",
      " [   6. 2006. 4006. 6006. 8006.]\n",
      " [   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]] [10014.] \n",
      "\n",
      "[[   1. 2001. 4001. 6001. 8001.]\n",
      " [   3. 2003. 4003. 6003. 8003.]\n",
      " [   5. 2005. 4005. 6005. 8005.]\n",
      " [   7. 2007. 4007. 6007. 8007.]\n",
      " [   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]] [10015.] \n",
      "\n",
      "[[   2. 2002. 4002. 6002. 8002.]\n",
      " [   4. 2004. 4004. 6004. 8004.]\n",
      " [   6. 2006. 4006. 6006. 8006.]\n",
      " [   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]] [10016.] \n",
      "\n",
      "[[   3. 2003. 4003. 6003. 8003.]\n",
      " [   5. 2005. 4005. 6005. 8005.]\n",
      " [   7. 2007. 4007. 6007. 8007.]\n",
      " [   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]] [10017.] \n",
      "\n",
      "[[   4. 2004. 4004. 6004. 8004.]\n",
      " [   6. 2006. 4006. 6006. 8006.]\n",
      " [   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]] [10018.] \n",
      "\n",
      "[[   5. 2005. 4005. 6005. 8005.]\n",
      " [   7. 2007. 4007. 6007. 8007.]\n",
      " [   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]] [10019.] \n",
      "\n",
      "[[   6. 2006. 4006. 6006. 8006.]\n",
      " [   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]] [10020.] \n",
      "\n",
      "[[   7. 2007. 4007. 6007. 8007.]\n",
      " [   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]] [10021.] \n",
      "\n",
      "[[   8. 2008. 4008. 6008. 8008.]\n",
      " [  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]] [10022.] \n",
      "\n",
      "[[   9. 2009. 4009. 6009. 8009.]\n",
      " [  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]] [10023.] \n",
      "\n",
      "[[  10. 2010. 4010. 6010. 8010.]\n",
      " [  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]] [10024.] \n",
      "\n",
      "[[  11. 2011. 4011. 6011. 8011.]\n",
      " [  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]] [10025.] \n",
      "\n",
      "[[  12. 2012. 4012. 6012. 8012.]\n",
      " [  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]] [10026.] \n",
      "\n",
      "[[  13. 2013. 4013. 6013. 8013.]\n",
      " [  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]] [10027.] \n",
      "\n",
      "[[  14. 2014. 4014. 6014. 8014.]\n",
      " [  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]] [10028.] \n",
      "\n",
      "[[  15. 2015. 4015. 6015. 8015.]\n",
      " [  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]] [10029.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(train_x_batches[0], train_y_batches[0]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second train batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  16. 2016. 4016. 6016. 8016.]\n",
      " [  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]] [10030.] \n",
      "\n",
      "[[  17. 2017. 4017. 6017. 8017.]\n",
      " [  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]] [10031.] \n",
      "\n",
      "[[  18. 2018. 4018. 6018. 8018.]\n",
      " [  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]] [10032.] \n",
      "\n",
      "[[  19. 2019. 4019. 6019. 8019.]\n",
      " [  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]] [10033.] \n",
      "\n",
      "[[  20. 2020. 4020. 6020. 8020.]\n",
      " [  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]] [10034.] \n",
      "\n",
      "[[  21. 2021. 4021. 6021. 8021.]\n",
      " [  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]] [10035.] \n",
      "\n",
      "[[  22. 2022. 4022. 6022. 8022.]\n",
      " [  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]] [10036.] \n",
      "\n",
      "[[  23. 2023. 4023. 6023. 8023.]\n",
      " [  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]] [10037.] \n",
      "\n",
      "[[  24. 2024. 4024. 6024. 8024.]\n",
      " [  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]\n",
      " [  36. 2036. 4036. 6036. 8036.]] [10038.] \n",
      "\n",
      "[[  25. 2025. 4025. 6025. 8025.]\n",
      " [  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]\n",
      " [  37. 2037. 4037. 6037. 8037.]] [10039.] \n",
      "\n",
      "[[  26. 2026. 4026. 6026. 8026.]\n",
      " [  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]\n",
      " [  36. 2036. 4036. 6036. 8036.]\n",
      " [  38. 2038. 4038. 6038. 8038.]] [10040.] \n",
      "\n",
      "[[  27. 2027. 4027. 6027. 8027.]\n",
      " [  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]\n",
      " [  37. 2037. 4037. 6037. 8037.]\n",
      " [  39. 2039. 4039. 6039. 8039.]] [10041.] \n",
      "\n",
      "[[  28. 2028. 4028. 6028. 8028.]\n",
      " [  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]\n",
      " [  36. 2036. 4036. 6036. 8036.]\n",
      " [  38. 2038. 4038. 6038. 8038.]\n",
      " [  40. 2040. 4040. 6040. 8040.]] [10042.] \n",
      "\n",
      "[[  29. 2029. 4029. 6029. 8029.]\n",
      " [  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]\n",
      " [  37. 2037. 4037. 6037. 8037.]\n",
      " [  39. 2039. 4039. 6039. 8039.]\n",
      " [  41. 2041. 4041. 6041. 8041.]] [10043.] \n",
      "\n",
      "[[  30. 2030. 4030. 6030. 8030.]\n",
      " [  32. 2032. 4032. 6032. 8032.]\n",
      " [  34. 2034. 4034. 6034. 8034.]\n",
      " [  36. 2036. 4036. 6036. 8036.]\n",
      " [  38. 2038. 4038. 6038. 8038.]\n",
      " [  40. 2040. 4040. 6040. 8040.]\n",
      " [  42. 2042. 4042. 6042. 8042.]] [10044.] \n",
      "\n",
      "[[  31. 2031. 4031. 6031. 8031.]\n",
      " [  33. 2033. 4033. 6033. 8033.]\n",
      " [  35. 2035. 4035. 6035. 8035.]\n",
      " [  37. 2037. 4037. 6037. 8037.]\n",
      " [  39. 2039. 4039. 6039. 8039.]\n",
      " [  41. 2041. 4041. 6041. 8041.]\n",
      " [  43. 2043. 4043. 6043. 8043.]] [10045.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(train_x_batches[1], train_y_batches[1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last train batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 560. 2560. 4560. 6560. 8560.]\n",
      " [ 562. 2562. 4562. 6562. 8562.]\n",
      " [ 564. 2564. 4564. 6564. 8564.]\n",
      " [ 566. 2566. 4566. 6566. 8566.]\n",
      " [ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]] [10574.] \n",
      "\n",
      "[[ 561. 2561. 4561. 6561. 8561.]\n",
      " [ 563. 2563. 4563. 6563. 8563.]\n",
      " [ 565. 2565. 4565. 6565. 8565.]\n",
      " [ 567. 2567. 4567. 6567. 8567.]\n",
      " [ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]] [10575.] \n",
      "\n",
      "[[ 562. 2562. 4562. 6562. 8562.]\n",
      " [ 564. 2564. 4564. 6564. 8564.]\n",
      " [ 566. 2566. 4566. 6566. 8566.]\n",
      " [ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]] [10576.] \n",
      "\n",
      "[[ 563. 2563. 4563. 6563. 8563.]\n",
      " [ 565. 2565. 4565. 6565. 8565.]\n",
      " [ 567. 2567. 4567. 6567. 8567.]\n",
      " [ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]] [10577.] \n",
      "\n",
      "[[ 564. 2564. 4564. 6564. 8564.]\n",
      " [ 566. 2566. 4566. 6566. 8566.]\n",
      " [ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]] [10578.] \n",
      "\n",
      "[[ 565. 2565. 4565. 6565. 8565.]\n",
      " [ 567. 2567. 4567. 6567. 8567.]\n",
      " [ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]] [10579.] \n",
      "\n",
      "[[ 566. 2566. 4566. 6566. 8566.]\n",
      " [ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]] [10580.] \n",
      "\n",
      "[[ 567. 2567. 4567. 6567. 8567.]\n",
      " [ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]] [10581.] \n",
      "\n",
      "[[ 568. 2568. 4568. 6568. 8568.]\n",
      " [ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]\n",
      " [ 580. 2580. 4580. 6580. 8580.]] [10582.] \n",
      "\n",
      "[[ 569. 2569. 4569. 6569. 8569.]\n",
      " [ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]\n",
      " [ 581. 2581. 4581. 6581. 8581.]] [10583.] \n",
      "\n",
      "[[ 570. 2570. 4570. 6570. 8570.]\n",
      " [ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]\n",
      " [ 580. 2580. 4580. 6580. 8580.]\n",
      " [ 582. 2582. 4582. 6582. 8582.]] [10584.] \n",
      "\n",
      "[[ 571. 2571. 4571. 6571. 8571.]\n",
      " [ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]\n",
      " [ 581. 2581. 4581. 6581. 8581.]\n",
      " [ 583. 2583. 4583. 6583. 8583.]] [10585.] \n",
      "\n",
      "[[ 572. 2572. 4572. 6572. 8572.]\n",
      " [ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]\n",
      " [ 580. 2580. 4580. 6580. 8580.]\n",
      " [ 582. 2582. 4582. 6582. 8582.]\n",
      " [ 584. 2584. 4584. 6584. 8584.]] [10586.] \n",
      "\n",
      "[[ 573. 2573. 4573. 6573. 8573.]\n",
      " [ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]\n",
      " [ 581. 2581. 4581. 6581. 8581.]\n",
      " [ 583. 2583. 4583. 6583. 8583.]\n",
      " [ 585. 2585. 4585. 6585. 8585.]] [10587.] \n",
      "\n",
      "[[ 574. 2574. 4574. 6574. 8574.]\n",
      " [ 576. 2576. 4576. 6576. 8576.]\n",
      " [ 578. 2578. 4578. 6578. 8578.]\n",
      " [ 580. 2580. 4580. 6580. 8580.]\n",
      " [ 582. 2582. 4582. 6582. 8582.]\n",
      " [ 584. 2584. 4584. 6584. 8584.]\n",
      " [ 586. 2586. 4586. 6586. 8586.]] [10588.] \n",
      "\n",
      "[[ 575. 2575. 4575. 6575. 8575.]\n",
      " [ 577. 2577. 4577. 6577. 8577.]\n",
      " [ 579. 2579. 4579. 6579. 8579.]\n",
      " [ 581. 2581. 4581. 6581. 8581.]\n",
      " [ 583. 2583. 4583. 6583. 8583.]\n",
      " [ 585. 2585. 4585. 6585. 8585.]\n",
      " [ 587. 2587. 4587. 6587. 8587.]] [10589.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(train_x_batches[-1], train_y_batches[-1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 600. 2600. 4600. 6600. 8600.]\n",
      " [ 602. 2602. 4602. 6602. 8602.]\n",
      " [ 604. 2604. 4604. 6604. 8604.]\n",
      " [ 606. 2606. 4606. 6606. 8606.]\n",
      " [ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]] [10614.] \n",
      "\n",
      "[[ 601. 2601. 4601. 6601. 8601.]\n",
      " [ 603. 2603. 4603. 6603. 8603.]\n",
      " [ 605. 2605. 4605. 6605. 8605.]\n",
      " [ 607. 2607. 4607. 6607. 8607.]\n",
      " [ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]] [10615.] \n",
      "\n",
      "[[ 602. 2602. 4602. 6602. 8602.]\n",
      " [ 604. 2604. 4604. 6604. 8604.]\n",
      " [ 606. 2606. 4606. 6606. 8606.]\n",
      " [ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]] [10616.] \n",
      "\n",
      "[[ 603. 2603. 4603. 6603. 8603.]\n",
      " [ 605. 2605. 4605. 6605. 8605.]\n",
      " [ 607. 2607. 4607. 6607. 8607.]\n",
      " [ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]] [10617.] \n",
      "\n",
      "[[ 604. 2604. 4604. 6604. 8604.]\n",
      " [ 606. 2606. 4606. 6606. 8606.]\n",
      " [ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]] [10618.] \n",
      "\n",
      "[[ 605. 2605. 4605. 6605. 8605.]\n",
      " [ 607. 2607. 4607. 6607. 8607.]\n",
      " [ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]] [10619.] \n",
      "\n",
      "[[ 606. 2606. 4606. 6606. 8606.]\n",
      " [ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]] [10620.] \n",
      "\n",
      "[[ 607. 2607. 4607. 6607. 8607.]\n",
      " [ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]] [10621.] \n",
      "\n",
      "[[ 608. 2608. 4608. 6608. 8608.]\n",
      " [ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]] [10622.] \n",
      "\n",
      "[[ 609. 2609. 4609. 6609. 8609.]\n",
      " [ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]] [10623.] \n",
      "\n",
      "[[ 610. 2610. 4610. 6610. 8610.]\n",
      " [ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]] [10624.] \n",
      "\n",
      "[[ 611. 2611. 4611. 6611. 8611.]\n",
      " [ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]] [10625.] \n",
      "\n",
      "[[ 612. 2612. 4612. 6612. 8612.]\n",
      " [ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]] [10626.] \n",
      "\n",
      "[[ 613. 2613. 4613. 6613. 8613.]\n",
      " [ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]] [10627.] \n",
      "\n",
      "[[ 614. 2614. 4614. 6614. 8614.]\n",
      " [ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]] [10628.] \n",
      "\n",
      "[[ 615. 2615. 4615. 6615. 8615.]\n",
      " [ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]] [10629.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[0], test_y_batches[0]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 616. 2616. 4616. 6616. 8616.]\n",
      " [ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]] [10630.] \n",
      "\n",
      "[[ 617. 2617. 4617. 6617. 8617.]\n",
      " [ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]] [10631.] \n",
      "\n",
      "[[ 618. 2618. 4618. 6618. 8618.]\n",
      " [ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]] [10632.] \n",
      "\n",
      "[[ 619. 2619. 4619. 6619. 8619.]\n",
      " [ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]] [10633.] \n",
      "\n",
      "[[ 620. 2620. 4620. 6620. 8620.]\n",
      " [ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]] [10634.] \n",
      "\n",
      "[[ 621. 2621. 4621. 6621. 8621.]\n",
      " [ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]] [10635.] \n",
      "\n",
      "[[ 622. 2622. 4622. 6622. 8622.]\n",
      " [ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]] [10636.] \n",
      "\n",
      "[[ 623. 2623. 4623. 6623. 8623.]\n",
      " [ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]] [10637.] \n",
      "\n",
      "[[ 624. 2624. 4624. 6624. 8624.]\n",
      " [ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]\n",
      " [ 636. 2636. 4636. 6636. 8636.]] [10638.] \n",
      "\n",
      "[[ 625. 2625. 4625. 6625. 8625.]\n",
      " [ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]\n",
      " [ 637. 2637. 4637. 6637. 8637.]] [10639.] \n",
      "\n",
      "[[ 626. 2626. 4626. 6626. 8626.]\n",
      " [ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]\n",
      " [ 636. 2636. 4636. 6636. 8636.]\n",
      " [ 638. 2638. 4638. 6638. 8638.]] [10640.] \n",
      "\n",
      "[[ 627. 2627. 4627. 6627. 8627.]\n",
      " [ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]\n",
      " [ 637. 2637. 4637. 6637. 8637.]\n",
      " [ 639. 2639. 4639. 6639. 8639.]] [10641.] \n",
      "\n",
      "[[ 628. 2628. 4628. 6628. 8628.]\n",
      " [ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]\n",
      " [ 636. 2636. 4636. 6636. 8636.]\n",
      " [ 638. 2638. 4638. 6638. 8638.]\n",
      " [ 640. 2640. 4640. 6640. 8640.]] [10642.] \n",
      "\n",
      "[[ 629. 2629. 4629. 6629. 8629.]\n",
      " [ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]\n",
      " [ 637. 2637. 4637. 6637. 8637.]\n",
      " [ 639. 2639. 4639. 6639. 8639.]\n",
      " [ 641. 2641. 4641. 6641. 8641.]] [10643.] \n",
      "\n",
      "[[ 630. 2630. 4630. 6630. 8630.]\n",
      " [ 632. 2632. 4632. 6632. 8632.]\n",
      " [ 634. 2634. 4634. 6634. 8634.]\n",
      " [ 636. 2636. 4636. 6636. 8636.]\n",
      " [ 638. 2638. 4638. 6638. 8638.]\n",
      " [ 640. 2640. 4640. 6640. 8640.]\n",
      " [ 642. 2642. 4642. 6642. 8642.]] [10644.] \n",
      "\n",
      "[[ 631. 2631. 4631. 6631. 8631.]\n",
      " [ 633. 2633. 4633. 6633. 8633.]\n",
      " [ 635. 2635. 4635. 6635. 8635.]\n",
      " [ 637. 2637. 4637. 6637. 8637.]\n",
      " [ 639. 2639. 4639. 6639. 8639.]\n",
      " [ 641. 2641. 4641. 6641. 8641.]\n",
      " [ 643. 2643. 4643. 6643. 8643.]] [10645.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[1], test_y_batches[1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second last test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 744. 2744. 4744. 6744. 8744.]\n",
      " [ 746. 2746. 4746. 6746. 8746.]\n",
      " [ 748. 2748. 4748. 6748. 8748.]\n",
      " [ 750. 2750. 4750. 6750. 8750.]\n",
      " [ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]] [10758.] \n",
      "\n",
      "[[ 745. 2745. 4745. 6745. 8745.]\n",
      " [ 747. 2747. 4747. 6747. 8747.]\n",
      " [ 749. 2749. 4749. 6749. 8749.]\n",
      " [ 751. 2751. 4751. 6751. 8751.]\n",
      " [ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]] [10759.] \n",
      "\n",
      "[[ 746. 2746. 4746. 6746. 8746.]\n",
      " [ 748. 2748. 4748. 6748. 8748.]\n",
      " [ 750. 2750. 4750. 6750. 8750.]\n",
      " [ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]] [10760.] \n",
      "\n",
      "[[ 747. 2747. 4747. 6747. 8747.]\n",
      " [ 749. 2749. 4749. 6749. 8749.]\n",
      " [ 751. 2751. 4751. 6751. 8751.]\n",
      " [ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]] [10761.] \n",
      "\n",
      "[[ 748. 2748. 4748. 6748. 8748.]\n",
      " [ 750. 2750. 4750. 6750. 8750.]\n",
      " [ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]] [10762.] \n",
      "\n",
      "[[ 749. 2749. 4749. 6749. 8749.]\n",
      " [ 751. 2751. 4751. 6751. 8751.]\n",
      " [ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]] [10763.] \n",
      "\n",
      "[[ 750. 2750. 4750. 6750. 8750.]\n",
      " [ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]] [10764.] \n",
      "\n",
      "[[ 751. 2751. 4751. 6751. 8751.]\n",
      " [ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]] [10765.] \n",
      "\n",
      "[[ 752. 2752. 4752. 6752. 8752.]\n",
      " [ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]] [10766.] \n",
      "\n",
      "[[ 753. 2753. 4753. 6753. 8753.]\n",
      " [ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]] [10767.] \n",
      "\n",
      "[[ 754. 2754. 4754. 6754. 8754.]\n",
      " [ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]] [10768.] \n",
      "\n",
      "[[ 755. 2755. 4755. 6755. 8755.]\n",
      " [ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]] [10769.] \n",
      "\n",
      "[[ 756. 2756. 4756. 6756. 8756.]\n",
      " [ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]] [10770.] \n",
      "\n",
      "[[ 757. 2757. 4757. 6757. 8757.]\n",
      " [ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]] [10771.] \n",
      "\n",
      "[[ 758. 2758. 4758. 6758. 8758.]\n",
      " [ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]] [10772.] \n",
      "\n",
      "[[ 759. 2759. 4759. 6759. 8759.]\n",
      " [ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]] [10773.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[-2], test_y_batches[-2]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 760. 2760. 4760. 6760. 8760.]\n",
      " [ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]] [10774.] \n",
      "\n",
      "[[ 761. 2761. 4761. 6761. 8761.]\n",
      " [ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]] [10775.] \n",
      "\n",
      "[[ 762. 2762. 4762. 6762. 8762.]\n",
      " [ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]] [10776.] \n",
      "\n",
      "[[ 763. 2763. 4763. 6763. 8763.]\n",
      " [ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]] [10777.] \n",
      "\n",
      "[[ 764. 2764. 4764. 6764. 8764.]\n",
      " [ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]] [10778.] \n",
      "\n",
      "[[ 765. 2765. 4765. 6765. 8765.]\n",
      " [ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]] [10779.] \n",
      "\n",
      "[[ 766. 2766. 4766. 6766. 8766.]\n",
      " [ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]] [10780.] \n",
      "\n",
      "[[ 767. 2767. 4767. 6767. 8767.]\n",
      " [ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]] [10781.] \n",
      "\n",
      "[[ 768. 2768. 4768. 6768. 8768.]\n",
      " [ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]\n",
      " [ 780. 2780. 4780. 6780. 8780.]] [10782.] \n",
      "\n",
      "[[ 769. 2769. 4769. 6769. 8769.]\n",
      " [ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]\n",
      " [ 781. 2781. 4781. 6781. 8781.]] [10783.] \n",
      "\n",
      "[[ 770. 2770. 4770. 6770. 8770.]\n",
      " [ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]\n",
      " [ 780. 2780. 4780. 6780. 8780.]\n",
      " [ 782. 2782. 4782. 6782. 8782.]] [10784.] \n",
      "\n",
      "[[ 771. 2771. 4771. 6771. 8771.]\n",
      " [ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]\n",
      " [ 781. 2781. 4781. 6781. 8781.]\n",
      " [ 783. 2783. 4783. 6783. 8783.]] [10785.] \n",
      "\n",
      "[[ 772. 2772. 4772. 6772. 8772.]\n",
      " [ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]\n",
      " [ 780. 2780. 4780. 6780. 8780.]\n",
      " [ 782. 2782. 4782. 6782. 8782.]\n",
      " [ 784. 2784. 4784. 6784. 8784.]] [10786.] \n",
      "\n",
      "[[ 773. 2773. 4773. 6773. 8773.]\n",
      " [ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]\n",
      " [ 781. 2781. 4781. 6781. 8781.]\n",
      " [ 783. 2783. 4783. 6783. 8783.]\n",
      " [ 785. 2785. 4785. 6785. 8785.]] [10787.] \n",
      "\n",
      "[[ 774. 2774. 4774. 6774. 8774.]\n",
      " [ 776. 2776. 4776. 6776. 8776.]\n",
      " [ 778. 2778. 4778. 6778. 8778.]\n",
      " [ 780. 2780. 4780. 6780. 8780.]\n",
      " [ 782. 2782. 4782. 6782. 8782.]\n",
      " [ 784. 2784. 4784. 6784. 8784.]\n",
      " [ 786. 2786. 4786. 6786. 8786.]] [10788.] \n",
      "\n",
      "[[ 775. 2775. 4775. 6775. 8775.]\n",
      " [ 777. 2777. 4777. 6777. 8777.]\n",
      " [ 779. 2779. 4779. 6779. 8779.]\n",
      " [ 781. 2781. 4781. 6781. 8781.]\n",
      " [ 783. 2783. 4783. 6783. 8783.]\n",
      " [ 785. 2785. 4785. 6785. 8785.]\n",
      " [ 787. 2787. 4787. 6787. 8787.]] [10789.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp,out in zip(test_x_batches[-1], test_y_batches[-1]):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Generator using `yield`\n",
    "Instead of using `return` statement, we can use `yield` statement, which is more memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train_x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "val_x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "val_y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "test_x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "test_y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    " \n",
    "\n",
    "def batch_generator(data, lookback, in_features, out_features, batch_size, step, min_ind, max_ind, future_y_val,\n",
    "                   norm=False, trim_last_batch=True):\n",
    "    \"\"\"\n",
    "    :param data: `ndarray`, input data.\n",
    "    :param lookback: `int`, sequence length, number of values LSTM will see at time `t` to make prediction at `t+1`.\n",
    "    :in_features: `int`, number of columns in `data` starting from 0 to be considered as input\n",
    "    :out_features: `int`, number of columns in `data` started from last to be considred as output/prediction.\n",
    "    :parm trim_last_batch: bool, if True, last batch will be ignored if that contains samples less than `batch_size`.\n",
    "    :parm norm: a dictionary which contains scaler object with which to normalize x and y data. We use separate scalers for x\n",
    "                 and y data. Keys must be `x_scaler` and `y_scaler`.\n",
    "    \"\"\"\n",
    "\n",
    "    # selecting the data of interest for x and y    \n",
    "    X = data[min_ind:max_ind, 0:in_features]\n",
    "    Y = data[min_ind:max_ind, -out_features:].reshape(-1,out_features)\n",
    "    \n",
    "    if norm:\n",
    "        x_scaler = norm['x_scaler']\n",
    "        y_scaler = norm['y_scaler']\n",
    "        X = x_scaler.fit_transform(X)\n",
    "        Y = y_scaler.fit_transform(Y)\n",
    "    \n",
    "    # container for keeping x and y windows. A `windows` is here defined as one complete set of data at one timestep.\n",
    "    x_wins = np.full((X.shape[0], lookback, in_features), np.nan, dtype=np.float32)\n",
    "    y_wins = np.full((Y.shape[0], out_features), np.nan)\n",
    "\n",
    "    # creating windows from X data\n",
    "    st = lookback*step - step # starting point of sampling from data\n",
    "    for j in range(st, X.shape[0]-lookback):\n",
    "        en = j - lookback*step\n",
    "        indices = np.arange(j, en, -step)\n",
    "        ind = np.flip(indices)\n",
    "        x_wins[j,:,:] = X[ind,:]\n",
    "\n",
    "    # creating windows from Y data\n",
    "    for i in range(0, Y.shape[0]-lookback):\n",
    "        y_wins[i,:] = Y[i+lookback,:]\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"removing trailing nans\"\"\"\n",
    "    first_nan_at_end = first_nan_from_end(y_wins[:,0])  # first nan in last part of data, start skipping from here\n",
    "    y_wins = y_wins[0:first_nan_at_end,:]\n",
    "    x_wins = x_wins[0:first_nan_at_end,:]\n",
    "\n",
    "    \"\"\"removing nans from start\"\"\"\n",
    "    y_val = st-lookback + future_y_val\n",
    "    if st>0:\n",
    "        x_wins = x_wins[st:,:]\n",
    "        y_wins = y_wins[y_val:,:]    \n",
    "\n",
    "    print(\"\"\"shape of x data: {} \\nshape of y data: {}\"\"\".format(x_wins.shape, y_wins.shape))\n",
    "\n",
    "    print(\"\"\".\\n{} values are skipped from start and {} values are skipped from end in output array\"\"\"\n",
    "          .format(st, X.shape[0]-first_nan_at_end))\n",
    "\n",
    "    pot_samples = x_wins.shape[0]\n",
    "\n",
    "    print('\\npotential samples are {}'.format(pot_samples))\n",
    "\n",
    "    residue = pot_samples % batch_size\n",
    "    print('\\nresidue is {} '.format(residue))\n",
    "\n",
    "    samples = pot_samples - residue\n",
    "    print('\\nActual samples are {}'.format(samples))\n",
    "\n",
    "    interval = np.arange(0, samples + batch_size, batch_size)\n",
    "    print('\\nPotential intervals: {}'.format(interval ))\n",
    "\n",
    "    interval = np.append(interval, pot_samples)\n",
    "    print('\\nActual interval: {} '.format(interval))\n",
    "\n",
    "    if trim_last_batch:\n",
    "        no_of_batches = len(interval)-2\n",
    "    else:\n",
    "        no_of_batches = len(interval) - 1 \n",
    "\n",
    "    print('\\nNumber of batches are {} '.format(no_of_batches))\n",
    "    \n",
    "    # code for generator\n",
    "    gen_i = 1\n",
    "    while 1:\n",
    "\n",
    "        for b in range(no_of_batches):\n",
    "            st = interval[b]\n",
    "            en = interval[b + 1]\n",
    "            x_batch = x_wins[st:en, :, :]\n",
    "            y_batch = y_wins[st:en]\n",
    "\n",
    "            gen_i +=1\n",
    "            \n",
    "            yield x_batch, y_batch\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_lookback=2  # sequence length\n",
    "input_features = 5\n",
    "output_features = 1\n",
    "_batch_size = 16\n",
    "input_stepsize = 2\n",
    "st_ind = 0\n",
    "end_ind = 600\n",
    "t_plus_ith_val = 1 # which value to predict in future, e.g if input is 11,12,13,14 and default value of this variable means we\n",
    "                  # want to predict 15, setting value equal to 3 means we want to predict 17.\n",
    "\n",
    "\n",
    "train_gen = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                                    input_stepsize, st_ind, end_ind, t_plus_ith_val,\n",
    "                                    norm={'x_scaler': train_x_scaler, 'y_scaler': train_y_scaler},\n",
    "                                    trim_last_batch = True) \n",
    "\n",
    "val_gen = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                                    input_stepsize,\n",
    "                          min_ind = 600,\n",
    "                          max_ind = 800,\n",
    "                          future_y_val = t_plus_ith_val,\n",
    "                          norm={'x_scaler': val_x_scaler, 'y_scaler': val_y_scaler},\n",
    "                          trim_last_batch = True) \n",
    "\n",
    "test_gen = batch_generator(data, _lookback, input_features, output_features, _batch_size,\n",
    "                        input_stepsize,\n",
    "                          min_ind = 800,\n",
    "                          max_ind = 1000,\n",
    "                          future_y_val = t_plus_ith_val,\n",
    "                           norm={'x_scaler': test_x_scaler, 'y_scaler': test_y_scaler},\n",
    "                          trim_last_batch = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x data: (196, 2, 5) \n",
      "shape of y data: (197, 1)\n",
      ".\n",
      "2 values are skipped from start and 2 values are skipped from end in output array\n",
      "\n",
      "potential samples are 196\n",
      "\n",
      "residue is 4 \n",
      "\n",
      "Actual samples are 192\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176 192]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 192 196] \n",
      "\n",
      "Number of batches are 12 \n",
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]] [0.01507538] \n",
      "\n",
      "[[0.00502513 0.00502513 0.00502513 0.00502513 0.00502513]\n",
      " [0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]] [0.0201005] \n",
      "\n",
      "[[0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]\n",
      " [0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]] [0.02512563] \n",
      "\n",
      "[[0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]\n",
      " [0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]] [0.03015075] \n",
      "\n",
      "[[0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]\n",
      " [0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]] [0.03517588] \n",
      "\n",
      "[[0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]\n",
      " [0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]] [0.04020101] \n",
      "\n",
      "[[0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]\n",
      " [0.040201   0.040201   0.040201   0.040201   0.040201  ]] [0.04522613] \n",
      "\n",
      "[[0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]\n",
      " [0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]] [0.05025126] \n",
      "\n",
      "[[0.040201   0.040201   0.040201   0.040201   0.040201  ]\n",
      " [0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]] [0.05527638] \n",
      "\n",
      "[[0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]\n",
      " [0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]] [0.06030151] \n",
      "\n",
      "[[0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]] [0.06532663] \n",
      "\n",
      "[[0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]] [0.07035176] \n",
      "\n",
      "[[0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]] [0.07537688] \n",
      "\n",
      "[[0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]] [0.08040201] \n",
      "\n",
      "[[0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]] [0.08542714] \n",
      "\n",
      "[[0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]] [0.09045226] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(val_gen)\n",
    "for inp,out in zip(x_batch, y_batch):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x data: (196, 2, 5) \n",
      "shape of y data: (197, 1)\n",
      ".\n",
      "2 values are skipped from start and 2 values are skipped from end in output array\n",
      "\n",
      "potential samples are 196\n",
      "\n",
      "residue is 4 \n",
      "\n",
      "Actual samples are 192\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176 192]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 192 196] \n",
      "\n",
      "Number of batches are 12 \n",
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]] [0.01507538] \n",
      "\n",
      "[[0.00502513 0.00502513 0.00502513 0.00502513 0.00502513]\n",
      " [0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]] [0.0201005] \n",
      "\n",
      "[[0.01005025 0.01005025 0.01005025 0.01005025 0.01005025]\n",
      " [0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]] [0.02512563] \n",
      "\n",
      "[[0.01507538 0.01507538 0.01507538 0.01507538 0.01507538]\n",
      " [0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]] [0.03015075] \n",
      "\n",
      "[[0.0201005  0.0201005  0.0201005  0.0201005  0.0201005 ]\n",
      " [0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]] [0.03517588] \n",
      "\n",
      "[[0.02512563 0.02512563 0.02512563 0.02512563 0.02512563]\n",
      " [0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]] [0.04020101] \n",
      "\n",
      "[[0.03015075 0.03015075 0.03015075 0.03015075 0.03015075]\n",
      " [0.040201   0.040201   0.040201   0.040201   0.040201  ]] [0.04522613] \n",
      "\n",
      "[[0.03517588 0.03517588 0.03517588 0.03517588 0.03517588]\n",
      " [0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]] [0.05025126] \n",
      "\n",
      "[[0.040201   0.040201   0.040201   0.040201   0.040201  ]\n",
      " [0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]] [0.05527638] \n",
      "\n",
      "[[0.04522613 0.04522613 0.04522613 0.04522613 0.04522613]\n",
      " [0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]] [0.06030151] \n",
      "\n",
      "[[0.05025126 0.05025126 0.05025126 0.05025126 0.05025126]\n",
      " [0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]] [0.06532663] \n",
      "\n",
      "[[0.05527638 0.05527638 0.05527638 0.05527638 0.05527638]\n",
      " [0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]] [0.07035176] \n",
      "\n",
      "[[0.06030151 0.06030151 0.06030151 0.06030151 0.06030151]\n",
      " [0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]] [0.07537688] \n",
      "\n",
      "[[0.06532663 0.06532663 0.06532663 0.06532663 0.06532663]\n",
      " [0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]] [0.08040201] \n",
      "\n",
      "[[0.07035176 0.07035176 0.07035176 0.07035176 0.07035176]\n",
      " [0.08040201 0.08040201 0.08040201 0.08040201 0.08040201]] [0.08542714] \n",
      "\n",
      "[[0.07537688 0.07537688 0.07537688 0.07537688 0.07537688]\n",
      " [0.08542714 0.08542714 0.08542714 0.08542714 0.08542714]] [0.09045226] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(test_gen)\n",
    "for inp,out in zip(x_batch, y_batch):\n",
    "    print(inp,out, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model using keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "tensorflow.set_random_seed(777)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1130 22:38:14.654478 554712 deprecation.py:506] From C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "W1130 22:38:15.346562 554712 deprecation.py:323] From C:\\Users\\USER\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "shape of x data: (596, 2, 5) \n",
      "shape of y data: (597, 1)\n",
      ".\n",
      "2 values are skipped from start and 2 values are skipped from end in output array\n",
      "\n",
      "potential samples are 596\n",
      "\n",
      "residue is 4 \n",
      "\n",
      "Actual samples are 592\n",
      "\n",
      "Potential intervals: [  0  16  32  48  64  80  96 112 128 144 160 176 192 208 224 240 256 272\n",
      " 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560\n",
      " 576 592]\n",
      "\n",
      "Actual interval: [  0  16  32  48  64  80  96 112 128 144 160 176 192 208 224 240 256 272\n",
      " 288 304 320 336 352 368 384 400 416 432 448 464 480 496 512 528 544 560\n",
      " 576 592 596] \n",
      "\n",
      "Number of batches are 37 \n",
      "500/500 [==============================] - 16s 33ms/step - loss: 0.0518 - val_loss: 0.0517\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0465 - val_loss: 0.0253\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0373 - val_loss: 0.0521\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0303 - val_loss: 0.0139\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0271 - val_loss: 0.0365\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0264 - val_loss: 0.0350\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0236 - val_loss: 0.0488\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0216 - val_loss: 0.0253\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0200 - val_loss: 0.0662\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0186 - val_loss: 0.0194\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0172 - val_loss: 0.0618\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0163 - val_loss: 0.0230\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0152 - val_loss: 0.0441\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0156 - val_loss: 0.0169\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0153 - val_loss: 0.0679\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.0149 - val_loss: 0.0300\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0145 - val_loss: 0.0464\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0144 - val_loss: 0.0330\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0139 - val_loss: 0.0344\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.0137 - val_loss: 0.0265\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.LSTM(64, input_shape=(_lookback, input_features), dropout=0.1, recurrent_dropout=0.5,))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                            steps_per_epoch=500,\n",
    "                            epochs=20,\n",
    "                            validation_data=val_gen,\n",
    "                            validation_steps=195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/sample\n",
      "[10835.] [10830.742]\n",
      "[10836.] [10831.669]\n",
      "[10837.] [10832.599]\n",
      "[10838.] [10833.531]\n",
      "[10839.] [10834.468]\n",
      "[10840.] [10835.407]\n",
      "[10841.] [10836.35]\n",
      "[10842.] [10837.295]\n",
      "[10843.] [10838.244]\n",
      "[10844.] [10839.195]\n",
      "[10845.] [10840.149]\n",
      "[10846.] [10841.107]\n",
      "[10847.] [10842.067]\n",
      "[10848.] [10843.03]\n",
      "[10849.] [10843.995]\n",
      "[10850.] [10844.963]\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y = next(test_gen)\n",
    "test_prediction = model.predict(test_x, 1, verbose=True)\n",
    "test_predict = test_y_scaler.inverse_transform(test_prediction)\n",
    "true = test_y_scaler.inverse_transform(test_y)\n",
    "for t,p in zip(true, test_predict):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
